---
title : 计算机网络
wiki: computer_basics
---

## 网络模型

>OSI 是理论上的网络通信模型，TCP/IP 是实际应用层面上的网络通信模型

通过分层标准化交互的过程。

具体讲，**分层**一方面使各层相互独立，不需要考虑其他层的实现，只需要调用下层功能；另一方面提高了灵活性和可替换性，类似高内聚、低耦合的原则；并且使得复杂的计算机网络系统变得易于设计，实现和标准化。

### `OSI`模型

`OSI`是一种**网络分层模型**，将网络通信过程分为七个层级，主要用于教学和标准化

1. **应用层**：为用户提供网络服务，如HTTP、DNS、文件传输（FTP）等。
2. **表示层**：负责数据的格式化、加密和解密，确保不同系统之间的数据能够互相理解。
3. **会话层**：管理设备之间的会话，确保通信过程中的数据流顺畅，并负责会话的建立、维护和终止（如RPC）。
4. **传输层**：提供端到端的数据传输，保证数据完整性和可靠性（如TCP、UDP协议）。
5. **网络层**：负责数据的路由选择和传输路径，确保数据从源主机到目的主机（如IP协议）。
6. **数据链路层**：保证数据在物理层上可靠地传输，处理错误检测与纠正（如以太网、MAC地址）。
7. **物理层**：负责数据的实际物理传输，包括电缆、光纤、无线信号等传输媒介。

---

### `TCP/IP`四层模型

是广泛应用的网络模型，是互联网的基础，可以将 TCP / IP 模型看作是 OSI 七层模型的精简版本

1. **应用层**：专注于提供应用功能，如HTTP、FTP、Telnet、DNS、SMTP    -应用数据
2. **传输层**：为应用层提供网络支持，包括TCP和UDP                                     -TCP头+应用数据
3. **网络层**：负责实际的传输功能，最常用的是IP协议                                     -IP头+TCP头+应用数据
4. **网络接口层**：为网络层提供传输服务，负责在底层网络发送原始数据包  -帧头+IP头+TCP头+应用数据+帧尾

网络接口层的传输单位是帧，IP层的传输单位是包，TCP的传输单位是段，HTTP的传输单位是消息或报文。

---

## HTTP

**HTTP**，即超文本传输协议，用于Web浏览器和服务器之间传输的协议，属于**应用层协议**。定义了浏览器如何请求网页，以及服务器如何响应这些请求。

**默认的端口**：80

>超文本：文字、图片、视频等的混合体，有超链接功能，最常见的：HTML

---

### 状态码

状态码用于描述请求的结果

1xx表示正在处理，2xx表示正常处理完毕，3xx表示需要附加操作，4xx表示服务器无法处理请求，5xx表示服务器处理请求出错

常见如，200 OK表示请求成功，204 No Content表示成功但是没有body数据，206 Partial Content表示资源只是一部分；

301Moved Permanently表示永久重定向，资源不存在了，需改用新的 URL，302 Found表示临时重定向，请求资源还在但是暂时需要另一个URL访问，304 Not Modified不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制；

403表示 Forbidden表示服务器紧张访问，404 Not Found表示请求的资源不存在；

500  Internal Server Error表示服务器内部错误，501表示请求的功能还不支持，502 Bad Gateway：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应，504 Gateway Time-out：为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器收到响应。

---

### 报文

**请求报文**：

请求行：包含请求方法、请求目标（URL或URI）和HTTP协议版本。

请求头部：包含关于请求的附加信息，如Host、User-Agent、Content-Type等。

空行：请求头部和请求体之间用空行分隔。

请求体：可选，包含请求的数据，通常用于POST请求等需要传输数据的情况。

**响应报文**：

状态行：包含HTTP协议版本、状态码和状态信息。

响应头部：包含关于响应的附加信息，如Content-Type、Content-Length等。

空行：响应头部和响应体之间用空行分隔。

响应体：包含响应的数据，通常是服务器返回的HTML、JSON等内容。

---

### 请求类型

- GET：用于请求获取指定资源，通常用于获取数据。
- POST：用于向服务器提交数据，通常用于提交表单数据或进行资源的创建。
- PUT：用于向服务器更新指定资源，通常用于更新已存在的资源。
- DELETE：用于请求服务器删除指定资源。
- HEAD：类似于GET请求，但只返回资源的头部信息，用于获取资源的元数据而不获取实际内容。

**GET vs POST**：

GET是从服务器获取指定的资源，请求的参数位置一般写在URL中，参数只允许ASCII，并且浏览器(不是HTTP)对URL长度有限制。GET是安全且幂等的，是“只读”操作，可被浏览器缓存。

POST是根据请求符合(报文body)对指定数据的资源做出处理，POST 请求携带数据的位置一般是写在报文 body 中，body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制。POST是不安全不幂等的，是“新增/提交数据”操作，会修改服务器资源，浏览器一般不会缓存POST请求。

注：理论上，任何请求都可以带 body 的，只是规范上GET不带body；并且URL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的。

---

### 常见字段

>**HTTP字段（HTTP field）是出现在HTTP请求或响应头部的一组`键: 值`结构，用于携带额外的指令或说明**，帮助服务器和客户端正确处理HTTP消息。

HOST：可以将请求发往「同一台」服务器上的不同网站

Content-Length：表明本次回应的数据长度

Connection：最常用于客户端要求服务器使用「HTTP 长连接」机制，以便其他请求复用

Content-Type：用于服务器回应时，告诉客户端，本次数据是什么格式

Content-Encoding：说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式

---

### 版本

**HTTP/1.0**：每次请求都要重新建立 TCP 连接，效率较低。它的最大问题是连接的**不持久性**。每个请求都必须等待前一个请求的响应完成后才能发送下一个请求。这种模式称为 **请求-响应阻塞**。

**HTTP/1.1**：最突出的特点是简单、灵活和易于扩展、应用广泛和跨平台，缺点是明文传输，不安全，无状态。**无状态**让服务器不用记忆HTTP状态，减轻服务器负担，但是坏处就是进行关联性操作的时候很麻烦，每次都要问身份信息什么的，无状态的解决方法简单的方法是使用Cookie技术，Cookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。浏览器会自动携带从服务器得到的 Cookie，并在后续请求中发送给服务器。**明文传输**：在传输过程的信息方便阅读，方便调试，但是内容可能被窃听，不安全。**不安全**：明文传输，容易被监听和篡改通信内容；缺乏请求验证，不会验证通信方的身份，容易受到伪造请求的攻击；无法证明报文的完整性。

**性能**：**长连接**（也叫持久连接），减少了重复TCP连接和断开的开销，只要一端没有明确提出断开连接，始终保持TCP连接状态，如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开这个连接。**管道网络传输**：(并没有广泛应用，有队头阻塞问题)建立在长连接的基础上，挂电脑网络传输使在同一个TCP连接里，客户端可以发起多个请求，不需要等前一个请求响应完，减少了整体的响应时间。**队头阻塞**：如果服务端在处理 A 请求时耗时比较长，那么后续的请.求的处理都会被阻塞住。

**性能瓶颈**：请求 / 响应头部信息不能压缩，只能压缩Body部分，头部冗长；队头阻塞；没有请求优先级控制；请求只能客户端发起服务器响应。

---

**HTTP/2**：基于HTTPS，安全性。

**性能改进**：**头部压缩**：协议会消除多个请求里相似的头的部分，即所谓的HPACK算法，客户端与服务器共同维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。**二进制格式**：不再是纯文本形式报文，而是全面采用二进制格式，头信息和数据体统称为帧：头信息帧，数据帧，计算机直接解析二进制报文，增加了效率。**并发传输**：引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接，解决队头阻塞的问题。针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应。**服务器推送**：服务器可以主动向客户端发消息，客户端和服务器都可以建立Stream，但是客户端的Stream ID必须是奇数，服务器必须是偶数。

**缺陷**：在TCP层存在“**队头阻塞**”问题：HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。

---

**HTTP/3**：把HTTP下层的TCP换成了UDP来解决HTTP2的队头阻塞问题。虽然UDP是不可靠传输，但是基于UDP的QUIC协议可以实现类似TCP的可靠传输。

**UDP的QUIC协议**：**无队头阻塞**：类似HTTP/2的 Stream与多路复用概念，在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。不同的是当某个流丢包只会阻塞这个流，因此不存在队头阻塞问题，不同于HTTP/2影响到其他流。**更快的建立连接**：在使用 HTTPS 的情况下，HTTP/1.1 和 HTTP/2 都是先进行 TCP 三次握手，再进行 TLS 握手，最后才开始传输加密的 HTTP 数据。 QUIC内部包含了TLS， 把 TCP 三次握手 + TLS 握手合并成一次。**连接迁移**：当客户端的网络发生变化（如从 Wi-Fi 切换到 5G），连接不会断，可以继续复用原连接进行通信。原理是使用连接ID代替传统的IP+端口标识连接，即便IP/端口改变，只要连接ID不变通信可以继续进行。

>HTTP/3.0 之前是基于 TCP 协议的，而 HTTP/3.0 将弃用 TCP，改用 **基于 UDP 的 QUIC 协议** 
>
>具体原因包括以下几点：
>
>1. **避免“队头阻塞”问题**
>    HTTP/2 虽然支持多路复用，但依赖于 TCP，而 TCP 是按字节流传输的。一旦某个数据包丢失，整个连接的所有流都要等待它重传，造成 **连接级的队头阻塞**。
>    QUIC 在传输层支持多路复用，每个流是独立传输的，**不会互相阻塞**，大大提升了并发请求效率。
>2. **减少连接建立时延**
>    TCP + TLS 通常需要至少 **3 次握手**才能建立加密连接，而 QUIC 集成了 TLS 1.3，**首次连接只需 1 次 RTT，重连甚至 0 RTT**，显著减少了连接建立时间。
>3. **支持更灵活的连接迁移**
>    QUIC 不再依赖 IP + 端口作为连接标识，而是使用连接 ID，这意味着**客户端在 IP 或网络切换时（如 WiFi ⇄ 4G），连接可以继续复用**，不需要重新握手。
>4. **更易于协议扩展和升级**
>    TCP 是操作系统协议栈的一部分，升级依赖内核。而 QUIC 基于用户态 UDP 协议，**可以灵活地在应用层进行更新和优化**，推动协议演进。

---

**HTTP/1.1 和 HTTP/2.0 的主要区别：**

1. **传输效率**
   - **HTTP/1.1**：同一域名下的连接默认是长连接（Keep-Alive），但一个 TCP 连接同一时刻只能处理一个请求，常见的优化方式是开启多个 TCP 连接，容易造成 **队头阻塞**。
   - **HTTP/2.0**：采用 **多路复用（Multiplexing）**，在一个 TCP 连接上可以同时发送多个请求和响应，避免了队头阻塞问题，大幅提高了传输效率。
2. **头部压缩**
   - **HTTP/1.1**：请求和响应头部是明文传输，且每次都要重复携带 Cookie、User-Agent 等冗余字段，浪费带宽。
   - **HTTP/2.0**：引入 **HPACK 压缩算法**，对头部进行压缩和复用，减少冗余数据，提高传输效率。
3. **二进制分帧**
   - **HTTP/1.1**：基于文本协议，请求和响应由文本格式组成，解析效率较低。
   - **HTTP/2.0**：采用 **二进制分帧**，所有传输的信息都会被切分为更小的帧，用帧来传输，效率更高且更灵活。
4. **服务器推送（Server Push）**
   - **HTTP/1.1**：客户端只能主动请求资源，服务器不能主动推送。
   - **HTTP/2.0**：服务器可以在客户端请求之前，主动推送可能需要的资源（比如 HTML 里引用的 CSS/JS），减少等待时间。

---

### 缓存技术

浏览器/中间服务器在本地保存资源副本，减少重复请求、提示加载速度、节省宽带

有两种实现方式：强制缓存和协商缓存

**强制缓存**：只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边（过期时间是服务器决定的）。是利用HTTP两个响应头部字段实现的，表示资源在客户端缓存的有效期：`Cache-Control`， 是一个相对时间（优先级高，选项多，设置精细，推荐）；`Expires`，是一个绝对时间。

**协商缓存**：协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。成功返回304。基于两种头部实现：**1**，请求头部中的 `If-Modified-Since` 字段与响应头部中的 `Last-Modified` 字段实现，浏览器用 `If-Modified-Since` 告诉服务器“我上次看到的修改时间”，服务器用 `Last-Modified` 判断资源是否变化，从而决定是否让浏览器继续用旧的；**2**，请求头部中的 `If-None-Match` 字段与响应头部中的 `ETag` 字段，用`ETag` 表示“资源版本”，浏览器每次带上旧版本号问服务器“这个版本还有效吗？”，服务器比对后决定是否返回新资源。**`ETag`优先级更高**，因为它可以解决Last-Modified的几个问题：内容没有修改的情况下文件最后修改时间也可能改变；可能有些文件是在秒级以内修改的，而`If-Modified-Since` 能检查到的粒度是秒级的；有些服务器不能精确获取文件的最后修改时间。

注：协商缓存是强制缓存的“备胎”，只有强制缓存失效时才会触发协商缓存流程。

 **`ETag` 实现协商缓存的过程**：

当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 `ETag` 唯一标识，这个唯一标识的值是根据当前请求的资源生成的。当浏览器再次请求访问服务器中的该资源时，首先会先检查强制缓存是否过期：如果没有过期，则直接使用本地缓存；如果缓存过期了，会在 Request 头部加上 If-None-Match 字段，该字段的值就是 `ETag` 唯一标识。

服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较：如果值相等，则返回 304 Not Modified，不会返回资源；如果不相等，则返回 200 状态码和资源内容，并在 Response 头部加上新的 `ETag`唯一标识。

如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。

### HTTP长连接

是一种允许在单个 TCP 连接上发送和接收多个 HTTP 请求/响应的技术。

它与早期 HTTP/1.0 的默认模式——**短连接**——形成对比。在 HTTP/1.1 中，长连接是**默认启用**的。

---

**如何工作的？**

1. **建立连接**：客户端（如浏览器）与服务器首先建立一个 TCP 连接（这个过程需要三次握手，有一定开销）。

2. **请求与响应**：客户端通过这个连接发送第一个请求，服务器返回第一个响应。

3. **保持连接打开**：在第一个请求/响应完成后，**双方都不关闭连接**。这个连接会保持一段时间的空闲状态，等待后续的请求。

4. **复用连接**：当客户端需要向**同一个服务器**发送下一个请求时（例如请求页面上的 CSS、JS、图片等资源），它直接使用这个已经建立好的空闲连接，而无需重新进行三次握手。

5. **关闭连接**：连接会在以下情况下关闭：

   空闲时间过长（通过 `Keep-Alive`timeout 参数控制）。

   任何一方主动要求关闭。

   发生错误。

---

**主要特点与优势**

1. **降低延迟**：消除了多次 TCP 握手（以及 TLS 握手，如果是 HTTPS）的开销，后续请求立即发送，页面加载更快。
2. **减少资源消耗**：减少了服务器和客户端因频繁建立和断开 TCP 连接而产生的 CPU 和内存消耗。
3. **提升网络效率**：允许 TCP 连接充分预热，克服了 TCP 慢启动的限制，从而更有效地利用可用网络带宽。
4. **实现流水线化**：在 HTTP/1.1 中，长连接是实现**请求流水线**（Pipelining）的基础，即可以同时发送多个请求而不必等待每个响应（尽管响应必须按顺序返回）。

---

**HTTP长连接对比WebSocket**

**全双工和半双工**：TCP 协议本身是全双工的，但我们最常用的 HTTP/1.1，虽然是基于 TCP 的协议，但它是半双工的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的 WebSocket 协议。

**应用场景区别**：在 HTTP/1.1 里，只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用定时轮询或者长轮询的方式实现服务器推送(comet)的效果。对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用 WebSocket 协议。

### HTTP断点重传

HTTP断点续传，也称为范围请求，是HTTP协议提供的一种优化机制。**它允许客户端只请求资源的一部分内容，而不是整个资源**。其核心价值在于**高效地恢复中断的传输**和**实现并行下载以提升速度**。这项技术是现代下载工具（如迅雷、IDM）和视频网站（实现进度条拖拽）的基础功能，能显著节省带宽并提升用户体验

它主要通过一对请求和响应头来**实现**：

1. **客户端**通过 `Range`头告知服务器需要的数据范围，例如 `Range: bytes=500-999`。
2. **服务器**如果支持，则返回状态码 `206 Partial Content`，并通过 `Content-Range`头告知返回的是哪个范围。服务器如果不支持，则依然会返回整个资源和 `200 OK`。

---

**工作流程：**

1. **探测支持情况**：聪明的客户端（如下载工具）可能会先发一个`HEAD`请求，通过检查响应头里是否有 `Accept-Ranges: bytes`来判断服务器是否支持此功能。
2. **中断与恢复**：假设一个10MB的文件下载到4MB时中断。恢复时，客户端不会重新下载整个文件，而是会发起一个新的GET请求，并带上请求头 `Range: bytes=4194304-`（4MB对应的字节偏移量）。
3. **服务器响应**：服务器收到后，会返回状态码 `206`和 `Content-Range: bytes 4194304-10485759/10485760`，并只发送从4MB到文件末尾的数据。
4. **客户端组装**：客户端将新下载的数据块与之前已下载的4MB文件拼接，得到完整文件

---

 **应用场景**

- 大文件下载（如视频、安装包）。
- 断网/暂停后继续下载，而不是重头开始。
- 多线程下载（多个线程同时请求不同的 Range，然后合并，提升速度）。

---

### 其他

{% folding child:codeblock open:false color:yellow HTTP1.1不拆包 %}

**HTTP/1.1 本身并不直接负责“拆包”**，它是基于 **TCP** 的应用层协议。真正的“拆包/粘包”问题发生在 **TCP 层**，而 HTTP/1.1 通过设计报文格式来解决如何正确地把 TCP 字节流还原成一条条 HTTP 请求。

HTTP/1.1 本身不做 TCP 层的拆包，而是通过 **Content-Length** 和 **Transfer-Encoding: chunked** 来标识请求/响应体的边界，从而让应用层能正确地从字节流中解析出一条条完整的消息。

{% endfolding %}



{% folding child:codeblock open:false color:yellow HTTP为什么不安全 %}

**1. 明文传输，容易被窃听**
 HTTP 协议传输的数据都是明文，攻击者只要在网络中间截获数据包，就能直接看到用户名、密码、Cookie 等敏感信息。

**2. 数据完整性无法保证**
 HTTP 没有校验和签名机制，数据在传输过程中可能被篡改，但客户端和服务端都无法感知，存在“中间人篡改”的风险。

**3. 无法验证通信双方身份**
 HTTP 本身没有认证机制，客户端无法确认服务端的真实身份，攻击者可以伪装成合法网站，实施钓鱼攻击；同样服务端也无法确认客户端身份。

{% endfolding %}



{% folding child:codeblock open:false color:yellow HTTP进行TCP连接之后，在什么情况下会中断 %}

当服务端或者客户端执行 close 系统调用的时候，会发送FIN报文，就会进行四次挥手的过程

当发送方发送了数据之后，接收方超过一段时间没有响应ACK报文，发送方重传数据达到最大次数的时候，就会断开TCP连接

当HTTP长时间没有进行请求和响应的时候，超过一定的时间，就会释放连接

{% endfolding %}



{% folding child:codeblock open:false color:yellow HTTP如何保存用户状态 %}

HTTP 协议是**无状态的协议**，它本身不会记录客户端的请求状态，每次请求都是独立的。这意味着服务器无法直接知道两个请求是否来自同一个用户。

为了在 HTTP 协议上实现用户状态的保存，通常会借助以下几种机制：

**方案一**

**Session (会话) 配合 Cookie (主流方式)：**

基本流程是这样的：

1. 用户向服务器发送用户名、密码、验证码用于登陆系统。
2. 服务器验证通过后，会为这个用户创建一个专属的 Session 对象（可以理解为服务器上的一块内存，存放该用户的状态数据，如购物车、登录信息等）存储起来，并给这个 Session 分配一个唯一的 `SessionID`。
3. 服务器通过 HTTP 响应头中的 `Set-Cookie` 指令，把这个 `SessionID` 发送给用户的浏览器。
4. 浏览器接收到 `SessionID` 后，会将其以 Cookie 的形式保存在本地。当用户保持登录状态时，每次向该服务器发请求，浏览器都会自动带上这个存有 `SessionID` 的 Cookie。
5. 服务器收到请求后，从 Cookie 中拿出 `SessionID`，就能找到之前保存的那个 Session 对象，从而知道这是哪个用户以及他之前的状态了。

使用 Session 的时候需要注意下面几个点：

- **客户端 Cookie 支持**：依赖 Session 的核心功能要确保用户浏览器开启了 Cookie。
- **Session 过期管理**：合理设置 Session 的过期时间，平衡安全性和用户体验。
- **Session ID 安全**：为包含 `SessionID` 的 Cookie 设置 `HttpOnly` 标志可以防止客户端脚本（如 JavaScript）窃取，设置 Secure 标志可以保证 `SessionID` 只在 HTTPS 连接下传输，增加安全性。

Session 数据本身存储在服务器端。常见的存储方式有：

- **服务器内存**:实现简单，访问速度快，但服务器重启数据会丢失，且不利于多服务器间的负载均衡。这种方式适合简单且用户量不大的业务场景。
- **数据库 (如 MySQL, PostgreSQL)**:数据持久化，但读写性能相对较低，一般不会使用这种方式。
- **分布式缓存 (如 Redis)**:性能高，支持分布式部署，是目前大规模应用中非常主流的方案。

**方案二**

**当 Cookie 被禁用时：URL 重写 (URL Rewriting)**

如果用户的浏览器禁用了 Cookie，或者某些情况下不便使用 Cookie，还有一种备选方案是 URL 重写。这种方式会将 `SessionID` 直接附加到 URL 的末尾，作为参数传递。例如：http://www.example.com/page?sessionid=xxxxxx。服务器端会解析 URL 中的 `sessionid` 参数来获取 `SessionID`，进而找到对应的 Session 数据。

这种方法一般不会使用，存在以下缺点：

- URL 会变长且不美观；
- `SessionID` 暴露在 URL 中，安全性较低（容易被复制、分享或记录在日志中）；
- 对搜索引擎优化 (SEO) 可能不友好。

**方案三**

**Token-based 认证 (如 JWT - JSON Web Tokens)**

这是一种越来越流行的无状态认证方式，尤其适用于前后端分离的架构和微服务。

以 JWT 为例（普通 Token 方案也可以），简化后的步骤如下

1. 用户向服务器发送用户名、密码以及验证码用于登陆系统；
2. 如果用户用户名、密码以及验证码校验正确的话，服务端会返回已经签名的 Token，也就是 JWT；
3. 客户端收到 Token 后自己保存起来（比如浏览器的 `localStorage` ）；
4. 用户以后每次向后端发请求都在 Header 中带上这个 JWT ；
5. 服务端检查 JWT 并从中获取用户相关信息。

{% endfolding %}



{% folding child:codeblock open:false color:yellow Cookie是HTTP协议簇的一部分，那为什么还说HTTP是无状态的 %}

HTTP 本质上是无状态协议，它不会记住客户端的状态；而 Cookie 只是利用请求头和响应头来传递状态信息，是对无状态的一种补充手段，但并没有改变 HTTP 协议本身的无状态特性。

{% endfolding %}



## HTTPS

### HTTP vs HTTPS

主要区别在于安全性，HTTP基于 **TCP** ，不加密，数据在传输过程是明文；HTTPS加密，通过SSL/TLS协议加密，SSL/TLS 位于 **HTTP** 和 **TCP** 之间，在传输过程不可被第三方轻易读取或篡改，HTTPS在TCP三次握手之后还需要SSL/TLS握手。

另外HTTP使用80端口，而HTTPS使用443端口；HTTPS耗费更多服务器资源。

**如何解决了HTTP的问题**

HTTP是明文存储，存在窃听风险，篡改风险，冒充风险。

HTTPS通过信息加密，校验机制，身份证书解决，具体：

**混合加密**：保证信息机密性，解决窃听风险。混合加密即**对称加密**和**非对称加密**：通信建立前采用非对称加密交换会话密钥，通信过程全部使用对称加密加密明文数据。对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。非对称加密使用两个密钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但是速度慢。

>你用锁（非对称加密）把一把钥匙（对称密钥）寄过去，只有朋友能打开锁（只有他有私钥）
>
>接下来你们用这把钥匙通信，聊天内容都上锁（对称加密），别人看不懂。

**摘要算法+数字签名**：摘要算法即哈希函数，哈希值唯一，且无法通过哈希值推导出内容。它可以保证内容不会被篡改，但是不能保证“内容+哈希值”不会被替换，因为缺少对客户端收到的信息是否来自服务器的证明。比如你可以同时伪造请假说明和家长签字来骗请假。那么就需要非对称加密算法解决：主要在于通过「私钥加密，公钥解密」的方式，来确认消息的身份吗，加密内容表示内容本身，因为耗费性能，是对哈希值加密。

>原文：我爱Java
>
>摘要：哈希后变成一串64位的码（假设为 `abc123`）
>
>私钥加密：把 `abc123` 用私钥加密生成签名
>
>发出去：把“我爱Java” + 签名一起发出
>
>接收方：
>
>用公钥解密签名 → 得到 `abc123`
>
>对“我爱Java”再哈希 → 也是 `abc123`
>
>二者相同 → OK，没改过，确实是发的人发的

**数字证书**：公钥可能被伪造，所以缺少身份验证的环节。通过一个权威机构证明身份（CA），只要证书可信，那么公钥可信。

>如果 你同时伪造请假说明和家长签字来骗请假的方法 被数字签名解决了，也就是家长和老师分别持有私钥和公钥，但是你也可以掉包两方的公私钥，那么家长可以把个人信息在警察局注册成数字证书「个人信息 + 公钥 + 数字签名」，那么老师拿到证书可以去警察局验证是否合法来判断这个公钥是不是家长的。

### SSL/TLS 协议

**SSL 和 TLS 没有太大的区别。**

SSL 指安全套接字协议（Secure Sockets Layer），首次发布与 1996 年。SSL 的首次发布其实已经是他的 3.0 版本，SSL 1.0 从未面世，SSL 2.0 则具有较大的缺陷（DROWN 缺陷——Decrypting RSA with Obsolete and Weakened eNcryption）。很快，在 1999 年，SSL 3.0 进一步升级，**新版本被命名为 TLS 1.0**。因此，TLS 是基于 SSL 之上的，但由于习惯叫法，通常把 HTTPS 中的核心加密协议混称为 SSL/TLS。

**工作原理**

SSL/TLS 的工作原理可以分为 **握手阶段**和 **数据传输阶段**：

**1. 握手阶段（Handshake）**

握手阶段的目的是**在客户端和服务器之间建立一个安全的通信通道**。主要过程如下：

1. **客户端发送客户端 Hello**：
    客户端发起连接，向服务器发送支持的加密算法、SSL/TLS 版本号、生成的随机数等信息。
2. **服务器响应服务器 Hello**：
    服务器收到客户端请求后，选择一种双方都支持的加密算法和 SSL/TLS 版本，返回给客户端。服务器还会发送服务器的数字证书（包含公钥）。
3. **客户端验证服务器证书**：
    客户端收到服务器的证书后，会使用证书颁发机构（CA）的公钥验证证书的有效性。如果证书有效，则继续进行，否则会终止连接。
4. **生成密钥（Pre-Master Secret）**：
    客户端生成一个对称加密密钥（称为 Pre-Master Secret），用服务器的公钥加密后发送给服务器。只有服务器才能解密得到这个密钥。
5. **双方生成会话密钥**：
    客户端和服务器根据 Pre-Master Secret 和双方各自的随机数，**通过密钥交换算法（如 Diffie-Hellman）生成相同的会话密钥**。这个密钥用于后续的数据加密。
6. **客户端和服务器交换 Finished 消息**：
    双方使用对称加密算法（如 AES）生成的会话密钥来加密后续的通信内容。在发送完握手消息后，握手阶段完成。

**2. 数据传输阶段（Data Transfer）**

在数据传输阶段，客户端和服务器使用**对称加密**来保护数据传输的机密性，确保数据不被第三方篡改或窃取。过程如下：

- 数据会使用之前交换的 **会话密钥** 进行加密传输。
- 每次数据传输时，SSL/TLS 会生成一个 **消息认证码（MAC）** 来验证数据的完整性，防止数据在传输过程中被修改。

### 如何建立连接

SSL/TLS协议流程：1，客户端向服务器索要并验证服务器的公钥，2，双方协商生产「会话秘钥」，3，双方采用「会话秘钥」进行加密通信。前两个阶段就是TLS握手阶段。

TLS握手涉及四次通信，现在常用的密钥交换算法有两种：RSA算法和ECDHE算法。

**TLS 协议建立详细流程**：

1，ClientHello：客户端向服务器发起ClientHello加密通信请求，发送了客户端支持的TLS协议版本 + 客户端生产的随机数(用来生成对话密钥) + 支持的密码套件系列(加密算法)

2，SeverHello：服务器做出响应，发送 确认TLS协议版本(不支持该版本就关闭加密通信) + 服务器生产的随机数(用来生成对话密钥) + 确认密码套件系列 + 服务器的数字证书

3，客户端回应：客户端收到回应后，先通过浏览器/操作系统的CA公钥确认数字证书真实性，随后从数字证书取出服务器公钥，使用它加密报文，向服务器发送 一个随机数(pre-master key,会被服务器公钥加密) + 加密通信算法改变通知(“随后的信息都将用「会话秘钥」加密通信”) + 客户端握手结束通知。同时把之前所有内容的发生的数据做摘要，原来供服务端校验。

双方使用三个随机数用协商的加密算法各自生成本次通信的会话密钥。

4，服务器最后回应：加密通信算法改变通知 + 握手结束通知

至此握手阶段结束，进入加密通信，完全使用HTTP协议，只不过使用会话密钥加密内容。

**CA 签发证书的过程**：1，CA把持有者公钥、用途、颁发者、有效时间等信息打包成一个包，对这些信息进行HASH计算，2，CA使用私钥将该hash值加密，生成Certificate Signature，也就是CA对证书进行了签名，3，将 Certificate Signature 添加在文件证书上，形成数字证书。

**客户端校验服务端的数字证书的过程**：1，客户端使用同样的Hash算法获取证书Hash值H1，2，通常浏览器和操作系统集成了CA公钥信息，浏览器收到证书后可以使用CA公钥解密Certificate Signature，得到哈希值H2，3，对比H1和H2，如果值相同则可信。

**证书信任链问题**：因为我们向CA申请的证书一般不是根证书，而是中间证书。1，当客户端收到证书后，发现签发者不是根证书，那就根据证书的签发者找到是“xx中间证书”，向CA请求中间证书，然后发现“xx中间证书”是“xx根证书CA”签发的，也就是说它是根证书，也就是自签证书。应用软件会检查此证书是否已经预载于根证书清单。如果有根证书，则用根证书公钥验证中间证书，通过后再用中间证书去验证收到的证书，依然通过后就可以信任。

**为什么要有证书链**：为了保证根证书的绝对安全，将根证书隔离，防止根证书失守导致整个信任链路的崩塌。

### 如何保证数据完整性

TLS分为**握手协议**和**记录协议**：握手协议即四次握手，负责协商加密算法和生成对应密钥；记录协议负责保护应用程序数据并验证完整性和来源。

**记录协议**：主要负责消息的压缩，加密，数据认证：1，消息被分割成多个较短的片段,然后分别对每个片段进行压缩；2，压缩后的片段会加上消息认证码(MAC)，这一步是为了保证完整性，及进行数据认证。同时为了防止重放攻击，在计算消息认证码时还加上了片段编码；3，片段再加上消息认证码会一起通过对称密码进行加密；4，经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。5，报文数据将传递到传输控制协议 (TCP) 层进行传输。

### 如何防范中间人

{% folding child:codeblock open:false color:yellow HTTPS是如何防范中间人的攻击 %}

主要通过加密和身份校验机制来防范中间人攻击的：

- 加密：https 握手期间会通过非对称加密的方式来协商出对称加密密钥。
- 身份校验：服务器会向证书颁发机构申请数字证书，证书中包含了服务器的公钥和其他相关信息。当客户端与服务器建立连接时，服务器会将证书发送给客户端。客户端会验证证书的合法性，包括检查证书的有效期、颁发机构的信任等。如果验证通过，客户端会使用证书中的公钥来加密通信数据，并将加密后的数据发送给服务器，然后由服务端用私钥解密。

中间人攻击的关键在于攻击者冒充服务器与客户端建立连接，并同时与服务器建立连接。

但由于攻击者无法获得服务器的私钥，因此无法正确解密客户端发送的加密数据。同时，客户端会在建立连接时验证服务器的证书，如果证书验证失败或存在问题，客户端会发出警告或中止连接。

{% endfolding %}



{% folding child:codeblock open:false color:yellow 如何避免被中间人抓取数据 %}

保证电脑安全；不点击证书非法的网站；HTTPS双向认证

**HTTPS双向认证**：一般的HTTPS是单向认证，服务端不会验证客户端身份。双向认证中客户端也有数字证书，并且提供给服务器，服务器要校验客户端身份。

{% endfolding %}

### 其他



{% folding child:codeblock open:false color:yellow 为什么抓包工具能截取 HTTPS 数据？ %}

工作原理与中间人一致，在服务端面前作为客户端，因为服务端不会校验客户端身份；在客户端面前作为服务端，因为它持有客户端的信任，也就是拥有对应域名的私钥。

客户端会往受系统信任的根证书列表导入抓包工具生成的证书，这个证书会被浏览器信任。

{% endfolding %}



## DNS

DNS（Domain Name System）域名管理系统，它是互联网中用于将域名转换为对应IP地址的分布式数据库系统。DNS 要解决的是**域名和 IP 地址的映射问题**。

目前 DNS 的设计采用的是分布式、层次数据库结构，**DNS 是应用层协议，它可以在 UDP 或 TCP 协议之上运行，端口为 53** 。

### DNS 服务器

DNS 服务器自底向上可以依次分为以下几个层级(所有 DNS 服务器都属于以下四个类别之一):

- 根 DNS 服务器。根 DNS 服务器提供 TLD 服务器的 IP 地址。目前世界上只有 13 组根服务器，我国境内目前仍没有根服务器。
- 顶级域 DNS 服务器（TLD 服务器）。顶级域是指域名的后缀，如`com`、`org`、`net`和`edu`等。国家也有自己的顶级域，如`uk`、`fr`和`ca`。TLD 服务器提供了权威 DNS 服务器的 IP 地址。
- 权威 DNS 服务器。在因特网上具有公共可访问主机的每个组织机构必须提供公共可访问的 DNS 记录，这些记录将这些主机的名字映射为 IP 地址。
- 本地 DNS 服务器。每个 ISP（互联网服务提供商）都有一个自己的本地 DNS 服务器。当主机发出 DNS 请求时，该请求被发往本地 DNS 服务器，它起着代理的作用，并将该请求转发到 DNS 层次结构中。严格说来，不属于 DNS 层级结构

世界上并不是只有 13 台根服务器，由于互联网的快速发展和增长，为了提高 DNS 的可靠性、安全性和性能，目前这 13 个 IP 地址中的每一个都有多个服务器，截止到 2023 年底，所有根服务器之和达到了 1700 多台，未来还会继续增加。

### DNS解析过程/工作流程

DNS解析的本质是将人类易于记忆的**域名**转换为机器用于路由的**IP地址**。其核心在于**分级查询与缓存机制**。

1. **本地查询与缓存检查**：
   - 当您在浏览器中输入一个网址（如`www.example.com`）时，操作系统会首先检查**本地缓存**（如浏览器缓存、操作系统缓存）中是否有该域名的解析结果。如果有且未过期，则直接返回，解析结束。
   - 若本地无缓存，则请求会发送至您网络配置中指定的**本地DNS解析器**（通常由您的ISP如电信、联通提供，或公共DNS如`114.114.114.114`、`8.8.8.8`）。
2. **递归解析器的工作**：
   - 本地DNS解析器收到请求后，首先也会检查自己的缓存。若没有，它便代表您的计算机开始一个**迭代查询**的过程，从DNS根服务器开始自上而下地查找。
3. **迭代查询流程（核心）**：
   - **查询根域名服务器**：本地DNS解析器首先向13组**根域名服务器**之一发起查询。根服务器不直接给出最终答案，而是返回负责顶级域`.com`的**顶级域名服务器**的IP地址列表。
   - **查询顶级域名服务器**：本地DNS解析器接着向`.com`顶级域名服务器查询。同样，该服务器返回负责`example.com`这个域的**权威域名服务器**的IP地址列表。
   - **查询权威域名服务器**：最后，本地DNS解析器向`example.com`的权威域名服务器查询`www.example.com`的IP地址。权威服务器是该域名信息的最终来源，它会返回确切的IP地址。
4. **返回结果与缓存**：
   - 本地DNS解析器收到最终的IP地址后，首先会将其**缓存**起来（根据TTL值设定缓存时间），以备后续查询使用。
   - 然后，它将解析结果返回给您的操作系统，操作系统同样会缓存，最终再将IP地址交给浏览器。
5. **发起连接**：
   - 浏览器此时终于拿到了目标服务器的IP地址，随即向该地址发起HTTP（S）请求，建立TCP连接，开始传输网页数据。

### DNS报文格式

DNS 报文分为查询和回答报文，两种形式的报文结构相同。

- 标识符。16 比特，用于标识该查询。这个标识符会被复制到对查询的回答报文中，以便让客户用它来匹配发送的请求和接收到的回答。
- 标志。1 比特的”查询/回答“标识位，`0`表示查询报文，`1`表示回答报文；1 比特的”权威的“标志位（当某 DNS 服务器是所请求名字的权威 DNS 服务器时，且是回答报文，使用”权威的“标志）；1 比特的”希望递归“标志位，显式地要求执行递归查询；1 比特的”递归可用“标志位，用于回答报文中，表示 DNS 服务器支持递归查询。
- 问题数、回答 RR 数、权威 RR 数、附加 RR 数。分别指示了后面 4 类数据区域出现的数量。
- 问题区域。包含正在被查询的主机名字，以及正被询问的问题类型。
- 回答区域。包含了对最初请求的名字的资源记录。**在回答报文的回答区域中可以包含多条 RR，因此一个主机名能够有多个 IP 地址。**
- 权威区域。包含了其他权威服务器的记录。
- 附加区域。包含了其他有帮助的记录。

### DNS 劫持

DNS 劫持是一种网络攻击，它通过修改 DNS 服务器的解析结果，使用户访问的域名指向错误的 IP 地址，从而导致用户无法访问正常的网站，或者被引导到恶意的网站。DNS 劫持有时也被称为 DNS 重定向、DNS 欺骗或 DNS 污染。这种劫持可以通过植入恶意的DNS记录或劫持用户的DNS流量来实现。

---

### 基于UDP

DNS 基于UDP协议实现，DNS使用UDP协议进行域名解析和数据传输。因为基于UDP实现DNS能够提供低延迟、简单快速、轻量级的特性，更适合DNS这种需要快速响应的域名解析服务。

- 低延迟：UDP是一种无连接的协议，不需要在数据传输前建立连接，因此可以减少传输时延，适合DNS这种需要快速响应的应用场景。
- 简单快速： UDP相比于TCP更简单，没有TCP的连接管理和流量控制机制，传输效率更高，适合DNS这种需要快速传输数据的场景。
- 轻量级：UDP头部较小，占用较少的网络资源，对于小型请求和响应来说更加轻量级，适合DNS这种频繁且短小的数据交换。

尽管 UDP 存在丢包和数据包损坏的风险，但在 DNS 的设计中，这些风险是可以被容忍的。DNS 使用了一些机制来提高可靠性，例如查询超时重传、请求重试、缓存等，以确保数据传输的可靠性和正确性。

---

## WebSocket

WebSocket 是一种基于 TCP 连接的全双工通信协议，即客户端和服务器可以同时发送和接收数据。WebSocket 协议本质上是应用层的协议，用于弥补 HTTP 协议在持久通信能力上的不足。客户端和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。

WebSocket继承了TCP 协议的全双工，并且解决了粘包的问题，因为它有明确的消息边界，每条信息都加了帧头，能让接收端准确解析完整消息。

使用场景：适用于需要服务器和客户端频繁交互的大部分场景

### WebSocket vs HTTP

WebSocket 是一种双向实时通信协议，而 HTTP 是一种单向通信协议。并且，HTTP 协议下的通信只能由客户端发起，服务器无法主动通知客户端。

WebSocket 使用 ws:// 或 wss://（使用 SSL/TLS 加密后的协议，类似于 HTTP 和 HTTPS 的关系） 作为协议前缀，HTTP 使用 http:// 或 https:// 作为协议前缀。

WebSocket 可以支持扩展，用户可以扩展协议，实现部分自定义的子协议，如支持压缩、加密等。

WebSocket 通信数据格式比较轻量，用于协议控制的数据包头部相对较小，网络开销小，而 HTTP 通信每次都要携带完整的头部，网络开销较大（HTTP/2.0 使用二进制帧进行数据传输，还支持头部压缩，减少了网络开销）。

### WebSocket vs 长、短轮询

**1.短轮询（Short Polling）**

- **原理**：客户端每隔固定时间（如 5 秒）发起一次 HTTP 请求，询问服务器是否有新数据。服务器收到请求后立即响应。
- **优点**：实现简单，兼容性好，直接用常规 HTTP 请求即可。
- 缺点
  - **实时性一般**：消息可能在两次轮询间到达，用户需等到下次请求才知晓。
  - **资源浪费大**：反复建立/关闭连接，且大多数请求收到的都是“无新消息”，极大增加服务器和网络压力。

**2.长轮询（Long Polling）**

- **原理**：客户端发起请求后，若服务器暂时无新数据，则会保持连接，直到有新数据或超时才响应。客户端收到响应后立即发起下一次请求，实现“伪实时”。
- 优点
  - **实时性较好**：一旦有新数据可立即推送，无需等待下次定时请求。
  - **空响应减少**：减少了无效的空响应，提升了效率。
- 缺点
  - **服务器资源占用高**：需长时间维护大量连接，消耗服务器线程/连接数。
  - **资源浪费大**：每次响应后仍需重新建立连接，且依然基于 HTTP 单向请求-响应机制。

**3. WebSocket**

- **原理**：客户端与服务器通过一次 HTTP Upgrade 握手后，建立一条持久的 TCP 连接。之后，双方可以随时、主动地发送数据，实现真正的全双工、低延迟通信。
- 优点
  - **实时性强**：数据可即时双向收发，延迟极低。
  - **资源效率高**：连接持续，无需反复建立/关闭，减少资源消耗。
  - **功能强大**：支持服务端主动推送消息、客户端主动发起通信。
- 缺点
  - **使用限制**：需要服务器和客户端都支持 WebSocket 协议。对连接管理有一定要求（如心跳保活、断线重连等）。
  - **实现麻烦**：实现起来比短轮询和长轮询要更麻烦一些。

### WebSocket vs SSE

SSE (Server-Sent Events) 和 WebSocket 都是用来实现服务器向浏览器实时推送消息的技术，让网页内容能自动更新，而不需要用户手动刷新。虽然目标相似，但它们在工作方式和适用场景上有几个关键区别：

1. 通信方式:
   - **SSE:** **单向通信**。只有服务器能向客户端（浏览器）发送数据。客户端不能通过同一个连接向服务器发送数据（需要发起新的 HTTP 请求）。
   - **WebSocket:** **双向通信 (全双工)**。客户端和服务器可以随时互相发送消息，实现真正的实时交互。
2. 底层协议:
   - **SSE:** 基于**标准的 HTTP/HTTPS 协议**。它本质上是一个“长连接”的 HTTP 请求，服务器保持连接打开并持续发送事件流。不需要特殊的服务器或协议支持，现有的 HTTP 基础设施就能用。
   - **WebSocket:** 使用**独立的 ws:// 或 wss:// 协议**。它需要通过一个特定的 HTTP "Upgrade" 请求来建立连接，并且服务器需要明确支持 WebSocket 协议来处理连接和消息帧。
3. 实现复杂度和成本:
   - **SSE:** **实现相对简单**，主要在服务器端处理。浏览器端有标准的 EventSource API，使用方便。开发和维护成本较低。
   - **WebSocket:** **稍微复杂一些**。需要服务器端专门处理 WebSocket 连接和协议，客户端也需要使用 WebSocket API。如果需要考虑兼容性、心跳、重连等，开发成本会更高。
4. 断线重连:
   - **SSE:** **浏览器原生支持**。EventSource API 提供了自动断线重连的机制。
   - **WebSocket:** **需要手动实现**。开发者需要自己编写逻辑来检测断线并进行重连尝试。
5. 数据类型:
   - **SSE:** **主要设计用来传输文本** (UTF-8 编码)。如果需要传输二进制数据，需要先进行 Base64 等编码转换成文本。
   - **WebSocket:** **原生支持传输文本和二进制数据**，无需额外编码。

为了提供更好的用户体验和利用其简单、高效、基于标准 HTTP 的特性，**Server-Sent Events (SSE) 是目前大型语言模型 API（如 OpenAI、DeepSeek 等）实现流式响应的常用甚至可以说是标准的技木选择**。



### WebSocket vs Socket

**关系可以表述为：WebSocket 协议是依托于 Socket API 和 TCP 连接来实现的。**

**1.Socket**

- **本质**：Socket 是**操作系统提供的一组 API**（应用程序编程接口），是 TCP/IP 协议族对外提供的编程接口。它并不是一个协议，而是位于**传输层**（TCP/UDP）之上的一层抽象。
- **作用**：它封装了复杂的网络协议细节，让开发者能够方便地使用 TCP 或 UDP 进行端到端的网络通信。它是构建所有网络应用的**基石**。
- **特点**：
  - 非常灵活和底层，你可以基于它实现任何自定义的应用层协议。
  - 需要开发者自己处理诸如数据粘包/拆包、心跳保活、协议格式等细节。
  - 通信模式通常是**全双工**的。

**简单说，Socket 是“工具”，提供了网络通信的基本能力。**

**2. WebSocket**

- **本质**：WebSocket 是一个完整的**应用层通信协议**（标准协议，RFC 6455），它依赖于 HTTP 进行初始握手，但之后便独立运行。
- **作用**：它旨在解决 HTTP 协议在**实时双向通信**上的缺陷（如轮询带来的低效和高延迟），为 Web 应用提供了一个真正的持久化、双向通信的解决方案。
- **特点**：
  - **基于 HTTP 握手**：连接建立通过一个 HTTP Upgrade 请求完成，兼容现有基础设施（如防火墙）。
  - **真正的双向通信**：连接建立后，服务器和客户端可以随时、主动地向对方发送数据，不再需要请求-响应模式。
  - **轻量级**：相比 HTTP，数据包头很小，通信效率高。
  - **协议内置**了心跳包（ping/pong）、帧格式定义等机制，开发者无需自己实现。

**简单说，WebSocket 是“方案”，是一个建立在 TCP 之上的、用于特定场景的高级通信协议。**



### 如何建立连接

客户端提供HTTP发送一个带有特殊头部的HTTP请求，请求升级为WebSocket协议；服务器如果支持WebSocket会返回101表示协议升级成功；HTTP通道升级为WebSocket连接，后续变成全双工的持久连接。

### 工作过程

WebSocket 的工作过程可以分为以下几个步骤：

1. 客户端向服务器发送一个 HTTP 请求，请求头中包含 `Upgrade: websocket` 和 `Sec-WebSocket-Key` 等字段，表示要求升级协议为 WebSocket；
2. 服务器收到这个请求后，会进行升级协议的操作，如果支持 WebSocket，它将回复一个 HTTP 101 状态码，响应头中包含 ，`Connection: Upgrade`和 `Sec-WebSocket-Accept: xxx` 等字段、表示成功升级到 WebSocket 协议。
3. 客户端和服务器之间建立了一个 WebSocket 连接，可以进行双向的数据传输。数据以帧（frames）的形式进行传送，WebSocket 的每条消息可能会被切分成多个数据帧（最小单位）。发送端会将消息切割成多个帧发送给接收端，接收端接收消息帧，并将关联的帧重新组装成完整的消息。
4. 客户端或服务器可以主动发送一个关闭帧，表示要断开连接。另一方收到后，也会回复一个关闭帧，然后双方关闭 TCP 连接。

另外，建立 WebSocket 连接之后，通过心跳机制来保持 WebSocket 连接的稳定性和活跃性。

### 消息格式

数据包在WebSocket叫做帧，主要关注以下字段：

**opcode字段**：标志数据帧的类型，比如1是text(String)，2是二进制数据类型([]byte)，8关闭连接

**payload字段**：存放想要传输的数据的长度(单位：字节)

**payload data字段**：真正要传输的数据

---

## 应用层其他

### token，session，cookie

**session**存储于服务器，可以理解为一个状态列表，拥有一个唯一识别符号sessionId，通常存放于cookie中。服务器收到cookie后解析出sessionId，再去session列表中查找，才能找到相应session，依赖cookie。

**cookie**类似一个令牌，装有sessionId，存储在客户端，浏览器通常会自动添加。

**token**也类似一个令牌，无状态，用户信息都被加密到token中，服务器收到token后解密就可知道是哪个用户，需要开发者手动添加。

---

**cookie vs session**

**存储位置：**Cookie的数据存储在客户端（通常是浏览器）。当浏览器向服务器发送请求时，会自动附带Cookie中的数据。Session的数据存储在服务器端。服务器为每个用户分配一个唯一的Session ID，这个ID通常通过Cookie或URL重写的方式发送给客户端，客户端后续的请求会带上这个Session ID，服务器根据ID查找对应的Session数据。

**数据容量：**单个Cookie的大小限制通常在4KB左右，而且大多数浏览器对每个域名的总Cookie数量也有限制。由于Session存储在服务器上，理论上不受数据大小的限制，主要受限于服务器的内存大小。

**安全性：**Cookie相对不安全，因为数据存储在客户端，容易受到XSS（跨站脚本攻击）的威胁。不过，可以通过设置HttpOnly属性来防止JavaScript访问，减少XSS攻击的风险，但仍然可能受到CSRF（跨站请求伪造）的攻击。Session通常认为比Cookie更安全，因为敏感数据存储在服务器端。但仍然需要防范Session劫持（通过获取他人的Session ID）和会话固定攻击。

**生命周期：**Cookie可以设置过期时间，过期后自动删除。也可以设置为会话Cookie，即浏览器关闭时自动删除。Session在默认情况下，当用户关闭浏览器时，Session结束。但服务器也可以设置Session的超时时间，超过这个时间未活动，Session也会失效。

**性能：**使用Cookie时，因为数据随每个请求发送到服务器，可能会影响网络传输效率，尤其是在Cookie数据较大时。使用Session时，因为数据存储在服务器端，每次请求都需要查询服务器上的Session数据，这可能会增加服务器的负载，特别是在高并发场景下。

{% folding child:codeblock open:true color:yellow 客户端禁用了cookie，session还能用吗 %}

默认情况下禁用 Cookie 后，Session 是无法正常使用的。因为大多数 Web 服务器都是依赖于 Cookie 来传递 Session 的会话 ID 的。客户端浏览器禁用 Cookie 时，服务器将无法把会话 ID 发送给客户端，客户端也无法在后续请求中携带会话 ID 返回给服务器，从而导致服务器无法识别用户会话。

但是，有几种方法可以绕过这个问题：

**URL重写：**每当服务器响应需要保持状态的请求时，将Session ID附加到URL中作为参数。这种方式的缺点是URL变得不那么整洁，且如果用户通过电子邮件或其他方式分享了这样的链接，可能导致Session ID的意外泄露。

**隐藏表单字段**：在每个需要Session信息的HTML表单中包含一个隐藏字段，用来存储Session ID。当表单提交时，Session ID随表单数据一起发送回服务器，服务器通过解析表单数据中的 Session ID 来获取用户的会话状态。这种方法仅适用于通过表单提交的交互模式，不适合链接点击或Ajax请求。



{% endfolding %}



{% folding child:codeblock open:true color:yellow 数据存储到 localStorage vs Cookie %}

**有什么区别**

存储容量: Cookie 的存储容量通常较小,每个 Cookie 的大小限制在几 KB 左右。而 LocalStorage 的存储容量通常较大,一般限制在几 MB 左右。因此,如果需要存储大量数据，LocalStorage 通常更适合;

数据发送: Cookie 在每次 HTTP 请求中都会自动发送到服务器,这使得 Cookie 适合用于在客户端和服务器之间传递数据。而 localStorage 的数据不会自动发送到服务器,它仅在浏览器端存储数据,因此 LocalStorage 适合用于在同一域名下的不同页面之间共享数据;

生命周期：Cookie 可以设置一个过期时间,使得数据在指定时间后自动过期。而 LocalStorage 的数据将永久存储在浏览器中,除非通过 JavaScript 代码手动删除;

安全性：Cookie 的安全性较低,因为 Cookie 在每次 HTTP 请求中都会自动发送到服务器,存在被窃取或篡改的风险。而 LocalStorage 的数据仅在浏览器端存储,不会自动发送到服务器,相对而言更安全一些;

**如何选择**

Cookie 适合用于在客户端和服务器之间传递数据、跨域访问和设置过期时间，而 LocalStorage 适合用于在同一域名下的不同页面之间共享数据、存储大量数据和永久存储数据。

{% endfolding %}



---

### JWT

JWT（JSON Web Token）是一种基于 JSON 格式的轻量级认证机制，常用于 **前后端分离、单点登录（SSO）、微服务认证**等场景。它本质上是一个 **加密签名的字符串**，服务端通过验证签名来判断用户身份，而不需要存储会话信息。

---

#### 字段

JWT令牌由三个部分组成：头部（Header）、载荷（Payload）和签名（Signature）。其中，头部和载荷均为JSON格式，使用Base64编码进行序列化，而签名部分是对头部、载荷和密钥进行签名后的结果。

---

#### 工作原理

1.用户登录成功后，服务端生成 JWT 并返回给客户端。

2.客户端保存 JWT（通常存储在 LocalStorage 或 Cookie 里）。

3.之后客户端每次请求时把 JWT 放在 **HTTP Header 的 Authorization 字段**中：

4.服务端收到请求后，使用密钥验证签名，确认合法性后提取用户信息。

---

#### 优点

- **无状态性**：JWT是无状态的令牌，不需要在服务器端存储会话信息。相反，JWT令牌中包含了所有必要的信息，如用户身份、权限等。这使得JWT在分布式系统中更加适用，可以方便地进行扩展和跨域访问。

- **安全性**：JWT使用密钥对令牌进行签名，确保令牌的完整性和真实性。只有持有正确密钥的服务器才能对令牌进行验证和解析。这种方式比传统的基于会话和Cookie的验证更加安全，有效防止了CSRF（跨站请求伪造）等攻击。

- **跨域支持**：JWT令牌可以在不同域之间传递，适用于跨域访问的场景。通过在请求的头部或参数中携带JWT令牌，可以实现无需Cookie的跨域身份验证。

---

#### 缺点

- **不可撤销**：一旦签发，在过期前无法轻易让它失效（除非维护黑名单）。
- **体积较大**：包含用户信息，放在每次请求的 Header 中，会增加网络开销。
- **Payload 明文**：虽然签名保证了完整性，但里面的数据是可解码的，敏感信息不能放里面。

---

#### JWT泄露

**泄露了怎么办**

及时失效令牌：当检测到JWT令牌泄露或存在风险时，可以立即将令牌标记为失效状态。服务器在接收到带有失效标记的令牌时，会拒绝对其进行任何操作，从而保护用户的身份和数据安全。

刷新令牌：JWT令牌通常具有一定的有效期，过期后需要重新获取新的令牌。当检测到令牌泄露时，可以主动刷新令牌，即重新生成一个新的令牌，并将旧令牌标记为失效状态。这样，即使泄露的令牌被恶意使用，也会很快失效，减少了被攻击者滥用的风险。

使用黑名单：服务器可以维护一个令牌的黑名单，将泄露的令牌添加到黑名单中。在接收到令牌时，先检查令牌是否在黑名单中，如果在则拒绝操作。这种方法需要服务器维护黑名单的状态，对性能有一定的影响，但可以有效地保护泄露的令牌不被滥用。

---

#### JWT存储

客户端收到服务器返回的 JWT，可以储存在 Local Storage 里面，也可以储存在Cookie里面，还可以存储在Session  Storage里面。

**Local Storage（本地存储）：**

优点：Local Storage 提供了较大的存储空间（一般为5MB），且不会随着HTTP请求一起发送到服务器，因此不会出现在HTTP缓存或日志中。

缺点：存在XSS（跨站脚本攻击）的风险，恶意脚本可以通过JavaScript访问到存储在Local Storage中的JWT，从而盗取用户凭证。

**Session Storage（会话存储）**

优点：与Local Storage类似，但仅限于当前浏览器窗口或标签页，当窗口关闭后数据会被清除，这在一定程度上减少了数据泄露的风险。

缺点：用户体验可能受影响，因为刷新页面或在新标签页打开相同应用时需要重新认证。

**Cookie**

优点：可以设置HttpOnly标志来防止通过JavaScript访问，减少XSS攻击的风险；可以利用Secure标志确保仅通过HTTPS发送，增加安全性。

缺点：大小限制较小（通常4KB），并且每次HTTP请求都会携带Cookie，可能影响性能；设置不当可能会受到CSRF（跨站请求伪造）攻击。

---

#### JWT 令牌为什么能解决集群部署

在传统的基于会话和Cookie的身份验证方式中，会话信息通常存储在服务器的内存或数据库中。但在集群部署中，不同服务器之间没有共享的会话信息，这会导致用户在不同服务器之间切换时需要重新登录，或者需要引入额外的共享机制（如Redis），增加了复杂性和性能开销。

而JWT令牌通过在令牌中包含所有必要的身份验证和会话信息，使得服务器无需存储会话信息，从而解决了集群部署中的身份验证和会话管理问题。当用户进行登录认证后，服务器将生成一个JWT令牌并返回给客户端。客户端在后续的请求中携带该令牌，服务器可以通过对令牌进行验证和解析来获取用户身份和权限信息，而无需访问共享的会话存储。

由于JWT令牌是自包含的，服务器可以独立地对令牌进行验证，而不需要依赖其他服务器或共享存储。这使得集群中的每个服务器都可以独立处理请求，提高了系统的可伸缩性和容错性。

---

### RPC

**RPC 的定义**
 RPC 是一种通信协议，它使得不同进程、不同机器上的程序能够像本地调用一样完成远程方法调用。在 Java 里比较典型的框架有 Dubbo 和 gRPC。

**核心原理**

1. **客户端调用代理（Stub）**：客户端发起调用时，先调用本地的代理对象。
2. **序列化请求**：将方法名、参数等信息序列化成二进制。
3. **网络传输**：通过网络（TCP/HTTP 等）发送给远程服务器。
4. **服务端反序列化**：服务端收到请求，反序列化得到方法和参数。
5. **执行方法**：服务端执行对应方法，并返回结果。
6. **客户端接收结果**：结果通过网络传回，客户端代理再反序列化，最终返回给调用方。

---

**为什么有HTTP协议了?还要用RPC?**

HTTP 是一种 **应用层协议**，它的目标是 **标准化客户端和服务端的通信**，让浏览器和服务器能互相理解。但它设计时的主要场景是 **文档传输**（网页、图片等资源），并不是专门为 **服务调用** 做的。

而在微服务或分布式架构里，我们更需要的是：

1. **高效的服务调用**：
   - HTTP 本身基于文本格式，协议头比较大，传输开销相对更高。
   - RPC 框架通常会采用 **二进制协议**（如 gRPC 基于 HTTP/2 + Protobuf），序列化和反序列化更快，性能更高。
2. **跨语言调用**：
   - RPC 框架天然支持多语言，比如 gRPC 就支持 Java、Go、Python 等语言间的通信；
   - 而 HTTP 只定义了传输方式，本身不解决序列化和接口规范问题。
3. **接口约束和工具链支持**：
   - RPC 一般基于 **IDL（接口描述语言）** 定义接口，比如 Protobuf，可以自动生成客户端和服务端代码，保证一致性；
   - HTTP 接口则需要手动维护接口文档和约定，容易出错。
4. **更丰富的特性**：
   - RPC 框架往往内置 **负载均衡、服务发现、熔断、重试** 等功能，更适合大规模分布式系统。
   - HTTP 需要额外配合 Nginx、注册中心等组件来实现。

---

## TCP

是面向连接的(必须一对一)、可靠的(保证一个报文一定到达接收端)、基于字节流(粘/拆包的“罪魁祸首”)的传输层通信协议

TCP 连接由**四元组**唯一标识：`源 IP + 源端口 + 目标 IP + 目标端口`

**为什么要有TCP**：IP层是不可靠的，不保证网络包的交付、按序交付、也不保证网络包的数据的完整性。那么就需要上层TCP协议来负责。TCP是一个工作在传输层的可靠数据传输的服务。

---

**TCP头格式**：序列号（解决网络包乱序问题），确认应答号（解决丢包问题），控制位

- 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。**用来解决网络包乱序问题。**

- 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。

- 控制位：

  - ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。

  - RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。
  - SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。
  - FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。

---

### TCP vs UDP

1，连接，TCP传输数据需要先建立连接；UDP不需要连接，即刻传输数据

2，服务对象，TCP一对一；UDP支持一对一，一对多，多对多

3，可靠性，TCP是可靠交付数据的，确保无差错 不丢失 不重复 按序到达；UDP是尽可能交付，不保证可靠（但是可以改造，比如QUIC）

4，拥塞控制、流量控制，TCP有拥塞控制和流量控制机制，保证数据的安全性；但是UDP都没有，网络非常拥堵也不影响UDP发送速率

5，首部开销，TCP首部较长，有一定开销；UDP首部只有8字节且固定不变

6，传输方式，TCP是流式传输，没有边界，但保证顺序；UDP是一个包一个包发送，有边界但是可能丢包和乱序

7，分片不同，TCP如果数据大小大于MSS，会在传输层分片，目标机也会在传输层组装，中途丢失一个只需要传输缺失的；UDP如果数据大小大于MSS，流程一样但是都是在IP层

8，应用，TCP常用于FTP文件传输，HTTP/HTTPS；UDP常用于较少通信，视频等多媒体通信，广播通信

---

选择 TCP 还是 UDP，主要取决于你的应用**对数据传输的可靠性要求有多高，以及对实时性和效率的要求有多高**。

当**数据准确性和完整性至关重要，一点都不能出错**时，通常选择 TCP。因为 TCP 提供了一整套机制（三次握手、确认应答、重传、流量控制等）来保证数据能够可靠、有序地送达。典型应用场景如下：

- **Web 浏览 (HTTP/HTTPS):** 网页内容、图片、脚本必须完整加载才能正确显示。
- **文件传输 (FTP, SCP):** 文件内容不允许有任何字节丢失或错序。
- **邮件收发 (SMTP, POP3, IMAP):** 邮件内容需要完整无误地送达。
- **远程登录 (SSH, Telnet):** 命令和响应需要准确传输。

当**实时性、速度和效率优先，并且应用能容忍少量数据丢失或乱序**时，通常选择 UDP。UDP 开销小、传输快，没有建立连接和保证可靠性的复杂过程。典型应用场景如下：

- **实时音视频通信 (VoIP, 视频会议, 直播):** 偶尔丢失一两个数据包（可能导致画面或声音短暂卡顿）通常比因为等待重传（TCP 机制）导致长时间延迟更可接受。应用层可能会有自己的补偿机制。
- **在线游戏:** 需要快速传输玩家位置、状态等信息，对实时性要求极高，旧的数据很快就没用了，丢失少量数据影响通常不大。
- **DHCP (动态主机配置协议):** 客户端在请求 IP 时自身没有 IP 地址，无法满足 TCP 建立连接的前提条件，并且 DHCP 有广播需求、交互模式简单以及自带可靠性机制。
- **物联网 (IoT) 数据上报:** 某些场景下，传感器定期上报数据，丢失个别数据点可能不影响整体趋势分析。

---

### 三次握手

#### 握手过程

[TCP 三次握手和四次挥手（传输层） | JavaGuide](https://javaguide.cn/cs-basics/network/tcp-connection-and-disconnection.html)

为了确保双方接收和发送能力都正常，建立一条可靠的全双工通信通道

1，一开始，客户端和服务端都处于 CLOSE 状态。先是服务端主动监听某个端口，处于 LISTEN 状态

2，客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态

3，服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态

4，客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 ESTABLISHED 状态

5，服务端收到客户端的应答报文后，也进入 ESTABLISHED 状态

6，一旦完成三次握手，双方都处于 ESTABLISHED 状态，此时连接就已建立完成，客户端和服务端就可以相互发送数据了

注：第三次握手可以顺便携带应用层数据，虽然不常见，但是协议允许；前两次不行，因为未完全建立连接，不安全

---

#### **为什么要三次？**

三次握手的本质是为了防止已失效的连接请求报文突然又传到服务端，从而造成错误的连接建立。

**两次握手的问题**
 如果只有两次：

- 客户端第一次发送的 SYN 因网络延迟滞留很久；
- 客户端以为超时了，就重新发了一个 SYN 并完成连接；
- 这时候延迟的旧 SYN 报文又到了服务端，服务端就会认为又有新连接，于是再建立一个无效连接，造成资源浪费。

**三次握手如何解决**

- 第三次握手（客户端 ACK）保证了**服务端收到的 SYN 报文是有效的、是最新的请求**；
- 避免了因为网络中“失效的 SYN”而导致错误的连接建立。

**四次握手**其实是把第二次握手的SYN和ACK分开发送给客户端，但是可以合并成一次。

---

#### **当握手丢失了会发生什么**

当第一次握手丢失：当客户端一直收不到第二次握手的回应，会触发超时重传机制，重新发送报文，并且与之前的序列号一样。（每次重传的超时时间是上次的2倍），如果还是没能收到服务端的第二次握手（SYN-ACK 报文），那么客户端就会断开连接。

当第二次握手丢失：因为第二次丢失对客户端的影响一样，所以也会触发上面的操作。同时对于服务器自然也收不到第三次握手，那么服务端会重传 SYN-ACK 报文，也就是第二次握手的超时重传。所以，当第二次握手丢失了，客户端和服务端都会重传。

当第三次握手丢失：服务端会重传 SYN-ACK 报文，当服务器重传到最大重传次数就断开连接。ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。

---

#### 半连接队列和全连接队列

在 TCP 三次握手过程中，Linux 内核会维护两个队列来管理连接请求：

**半连接队列**（也称 SYN Queue）：当服务端收到客户端的 SYN 请求时，此时双方还没有完全建立连接，它会把半连接状态的连接放在半连接队列。

**全连接队列**（也称 Accept Queue）：当服务端收到客户端对 ACK 响应时，意味着三次握手成功完成，服务端会将该连接从半连接队列移动到全连接队列。如果未收到客户端的 ACK 响应，会进行重传，重传的等待时间通常是指数增长的。如果重传次数超过系统规定的最大重传次数，系统将从半连接队列中删除该连接信息。

这两个队列的存在是为了处理并发连接请求，确保服务端能够有效地管理新的连接请求。

不管是半连接队列还是全连接队列，都有最大长度限制，超过限制时，内核会直接丢弃，或返回 RST 包

---

#### 第三次握手可以携带数据

在 TCP 三次握手过程中，第三次握手是可以携带数据的(客户端发送完 ACK 确认包之后就进入 ESTABLISHED 状态了)，如果第三次握手的 ACK 确认包丢失，但是客户端已经开始发送携带数据的包，那么服务端在收到这个携带数据的包时，如果该包中包含了 ACK 标记，服务端会将其视为有效的第三次握手确认。这样，连接就被认为是建立的，服务端会处理该数据包，并继续正常的数据传输流程

---

#### 服务器内部发生了什么

服务端收到客户端发起的 SYN 请求后，**内核会把该连接存储到半连接队列**，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，**内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来**。

---

#### 大量SYN包发送给服务端会发生什么事情

有可能会导致TCP 半连接队列放满，这样**当 TCP 半连接队列满了，后续再在收到 SYN 报文就会丢弃**，导致客户端无法和服务端建立连接。

**SYN 攻击**是攻击者向服务器**伪造大量TCP连接请求（SYN包）**，并且在收到服务器的回应（SYN-ACK）后，**拒绝发送最终的确认（ACK）**。这会导致服务器上积压大量半连接，耗尽其资源，使其无法为正常用户提供服务。

避免 SYN 攻击方式，可以有以下四种方法：

- 调大 netdev_max_backlog；
- 增大 TCP 半连接队列；
- 开启 tcp_syncookies；
- 减少 SYN+ACK 重传次数

------

#### 为什么每次建立TCP连接初始化的序列号都要求不一样？

是为了防止旧数据混入新连接、避免重放攻击，并确保每个连接的独立性和安全性。

假设你第一次和服务器建立连接时，序列号为 `0`，并传输了数据。后来，连接关闭了，但是某些数据包在网络中滞留。然后你再次建立连接，如果序列号没有变化，滞留的旧数据可能会被服务器误认为是新连接的数据，导致数据混淆或错误处理。通过随机化序列号，服务器可以确保新连接的序列号与旧连接的序列号不同，从而避免这个问题。

并且攻击者可以猜测和伪造数据包，进行**TCP 重放攻击**。

**ISN序列号是怎么随机产生的？**

TCP 的初始序列号 ISN 是通过**系统内核根据时间和连接信息计算出的伪随机数**，确保每次连接的序列号不同且难以被攻击者预测，从而保障连接的正确性和安全性。

如Linux是   随着时间 每 4 微秒递增的全局计数器   +  基于本地地址、远程地址、端口、时间戳等信息的哈希函数。

---

#### 第 2 次握手传回了 ACK，为什么还要传回 SYN

**SYN** 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务端之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务端使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 **ACK**(Acknowledgement）消息响应。这样在客户机和服务端之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务端之间传递。

服务端传回发送端所发送的 ACK 是为了告诉客户端：“我接收到的信息确实就是你所发送的信号了”，这表明从客户端到服务端的通信是正常的。回传 SYN 则是为了建立并确认从服务端到客户端的通信

---

### 四次挥手

#### 挥手过程

四次挥手是 TCP 双方各自独立关闭发送通道的过程，确保数据传输完整、有序关闭连接，其中 FIN 表示主动关闭，ACK 表示确认关闭。

1，客户端主动调用关闭连接的函数，向服务器发送FIN报文段，表示没有数据要发送，但是还能接收数据，进入FIN_WAIT_1状态

2，服务器向客户端发送ACK报文段，表示知道客户端不发数据了，进入CLOSE_WAIT状态。在收到 FIN 报文的时候，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，服务端应用程序可以通过 read 调用来感知这个 FIN 包，这个 EOF 会被**放在已排队等候的其他已接收的数据之后**，所以必须要得继续 read 接收缓冲区已接收的数据；

3，接着，当服务端在 read 数据的时候，最后自然就会读到 EOF，接着 **read() 就会返回 0，这时服务端应用程序如果有数据要发送的话，就发完数据后才调用关闭连接的函数，如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数**，这时服务端就会发一个 FIN 包，这个 FIN 报文代表服务端不会再发送数据了，之后处于 LAST_ACK 状态

4，客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；

5，服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；

6，客户端经过 2MSL(2*报文最大生存时间) 时间之后，也进入 CLOSE 状态；

双方都可以主动断开连接，但是主动关闭连接的，才有 TIME_WAIT 状态

---

#### 为什么需要四次挥手

服务器收到客户端的 FIN 报文时，内核会马上回一个 ACK 应答报文，但是服务端应用程序可能还有数据要发送，所以并不能马上发送 FIN 报文，而是将发送 FIN 报文的控制权交给服务端应用程序，所以服务端的ACK和FIN一般分开发送，因此是四次挥手。

当被动关闭方在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。

----

#### 挥手丢失

第一次挥手丢失：客户端迟迟收不到服务端ACK，触发超时重传，重发报文，重发次数超过tcp_orphan_retries后会等待一段时间( *=2 )，如果还是没有收到就直接进入close状态

第二次挥手丢失：ACK 报文是不会重传的，客户端重复第一次挥手丢失的操作

注：当客户端处于FIN_WAIT2状态等服务端发送第三次挥手的时候，如果你使用的`close()`关闭连接，`FIN_WAIT2` 状态最多持续 `tcp_fin_timeout` 秒（默认 60 秒）；但如果用 `shutdown()` 只关闭发送方向，`tcp_fin_timeout` 不生效，连接可能永远停在 `FIN_WAIT2` 状态，造成资源泄漏。

第三次挥手丢失：客户端一直停在 `FIN_WAIT2` 状态，等不到第三次挥手，若客户端使用 `close()`，`tcp_fin_timeout` 参数会控制 `FIN_WAIT2` 的最大时长。服务端会在超时重传 `FIN` 报文，重传超出系统设定的上限关闭连接

第四次挥手丢失：客户端收到FIN报文后进入TIME_WAIT状态。在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭状态。但是服务器没有收到ACK报文，重复第三次挥手丢失的操作(重传+断开连接)。这个时候如果客户端又收到了超时重传的FIN，就会重置定时器，等待2MSL时长，断开连接。

---

#### 为什么要等2MSL

第四次挥手时，客户端发送给服务端的 ACK 有可能丢失，如果服务端因为某些原因而没有收到 ACK 的话，服务端就会重发 FIN，如果客户端在 2*MSL 的时间内收到了 FIN，就会重新发送 ACK 并再次等待 2MSL，防止 Server 没有收到 ACK 而不断重发 FIN。

相当于至少允许报文丢失一次。

1，防止因为第四次挥手的ACK丢失导致对方收不到ACK；2， 防止旧连接报文干扰新连接：双方都不再发数据，但是不能保证数据都已经送达，所以会有旧数据在网络中，如果新连接使用了相同的四元组（IP+端口），旧包到达时可能被错误当作新连接的数据，等待2MSL是为了确保数据包彻底过期。

---

### 如何保证传输的可靠性

TCP 通过一系列机制来保证数据传输的可靠性。首先，TCP 是面向连接的协议，在传输前必须建立一个可靠的连接，即 **三次握手**，保证双方能够通信。数据传输过程中，TCP 会使用 **序列号** 和 **确认号** 来确保数据按顺序到达。

具体来说，TCP 保证可靠性的方式包括：

1. **数据分段与重组**：大数据会被分割成小数据包传输，每个数据包都有序列号，接收方可以按序号重新组合数据，确保数据的顺序性。
2. **确认机制**：每当接收方收到数据包时，都会向发送方发送一个确认消息，告诉发送方自己已成功接收到数据。
3. **重传机制**：如果发送方没有在规定时间内收到确认消息，或者接收到的确认号不符合预期，TCP 会重新发送未确认的数据包，确保数据不丢失。
4. **流量控制**：通过滑动窗口机制，TCP 控制发送方的数据发送速度，避免接收方处理不过来导致丢包。
5. **拥塞控制**：通过算法如慢开始、拥塞避免、快速重传等，TCP 在网络出现拥塞时，动态调整发送速率，避免网络负载过重导致的数据丢失。

这些机制共同作用，确保了 TCP 数据传输的可靠性。

---

### 如何实现流量控制？

TCP 实现流量控制的主要机制是 **滑动窗口**。流量控制的目的是防止发送方发送的数据超过接收方的处理能力，避免数据丢失。

具体来说，TCP 通过以下方式实现流量控制：

1. **接收窗口**：每个 TCP 连接都有一个接收窗口，表示接收方的缓存空间大小。接收方通过 **窗口大小** 告诉发送方自己当前能接受的最大数据量。窗口的大小会动态变化，反映接收方的处理能力和缓冲区空间。
2. **滑动窗口机制**：发送方根据接收方反馈的窗口大小来控制数据的发送量。接收方窗口的大小决定了发送方能在没有确认的情况下发送的数据量。**窗口的滑动**意味着随着接收方处理完部分数据，接收窗口逐渐“打开”，发送方可以继续发送更多数据。
3. **调整发送速率**：如果接收方的窗口大小变小（比如缓冲区已满），发送方会减少数据发送的速率，避免产生溢出或丢包。

通过滑动窗口机制，TCP 能够根据接收方的处理能力动态调整发送的数据量，确保数据传输不会因为接收方处理能力不足而丢失或造成过载。

---

### 拥塞控制是怎么实现的

TCP 的拥塞控制通过动态调整数据发送速率来避免网络拥塞，确保网络的稳定性。TCP 使用了四种主要的拥塞控制算法：

1. **慢启动（Slow Start）**：
    当连接开始时，TCP 发送方会将拥塞窗口设置为一个较小的值（通常是 1 或 2 个最大段大小 MSS）。每当收到一个确认（ACK），拥塞窗口就增加 1 个 MSS，直到达到一个阈值，进入后续阶段。慢启动的目的是迅速探测网络的带宽，避免初期发送过多数据导致拥塞。
2. **拥塞避免（Congestion Avoidance）**：
    当拥塞窗口大小超过慢启动阈值时，进入拥塞避免阶段。此时，TCP 不再以指数增长的方式增加窗口，而是采用线性增长。每收到一个确认，窗口增大 1/MSS，窗口增大速度减缓，以避免过快增加负载导致拥塞。
3. **快速重传（Fast Retransmit）**：
    当发送方连续收到 3 个重复的 ACK 时，表示某个数据包丢失了。此时，TCP 立即重传丢失的数据包，而无需等待超时，从而减少了重传的延迟。
4. **快速恢复（Fast Recovery）**：
    在快速重传后，TCP 进入快速恢复阶段。此时，拥塞窗口大小被设置为丢失数据包时窗口大小的一半，继续通过线性增长的方式进行拥塞避免。这使得 TCP 可以迅速恢复并继续传输数据，而不是重新进入慢启动阶段。

---

### 粘包怎么解决

粘包的问题出现是因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。

一般有三种方式分包的方式：

- 固定长度的消息；
- 特殊字符作为边界；
- 自定义消息结构。

**固定长度的消息**：最简单方法，但是灵活性不高，实际中很少用。每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。

**特殊字符作为边界**：在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息，需要注意转义。HTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界。

**自定义消息结构**：可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。

---

### 拥塞控制

TCP拥塞控制是TCP协议的一套**核心算法机制**，其根本目的是**感知网络的拥堵状况，并动态调整自己的数据发送速率**，以避免网络因过载而瘫痪，同时在所有共享网络资源的连接之间实现一种公平和高效的带宽利用。

---

#### 有什么用

要理解拥塞控制，首先要明白它解决的是什么问题：

1. **“公地悲剧”**：网络带宽是一种共享的公共资源。如果所有TCP连接都不顾一切地以最高速率发送数据，就会导致网络核心的路由器队列被填满，进而引发大量数据包被丢弃。
2. **恶性循环**：数据包丢失会触发超时重传，而重传又会加剧网络拥堵，最终导致网络吞吐量急剧下降，甚至“卡死”，这种情况称为**拥塞崩溃**。
3. **与流量控制的区别**：流量控制（Sliding Window）解决的是**发送方和接收方之间**的速度不匹配问题（接收方缓冲区溢出）。而拥塞控制解决的是**发送方与整个网络之间**的速度不匹配问题（网络路由器缓冲区溢出）。

**因此，拥塞控制的本质是：TCP发送方扮演一个“谦谦君子”，通过不断试探和调整，找到一个既快又不给网络添麻烦的最优发送速率。**

---

#### 控制算法

>简单版：首先是**慢启动**，连接开始时以指数增长方式快速探测带宽。当窗口达到阈值`ssthresh`后，进入**拥塞避免**阶段，转为线性增长，缓慢逼近网络容量极限。
>
>当出现拥塞时，有两种处理方式：如果发生超时，说明拥塞严重，TCP会大幅降低速率并重新慢启动；如果收到**3个重复ACK**，则会触发**快速重传**立即补发数据，并进入**快速恢复**阶段，温和地减半速率后直接进入拥塞避免，避免了性能陡降。
>
>整个过程的核心是围绕**拥塞窗口（cwnd）** 和**慢启动阈值（ssthresh）** 两个状态变量，通过**AIMD（加法增大、乘法减小）** 的原则来公平且高效地利用网络带宽。

TCP拥塞控制是一个由**四个核心算法**组成的复合机制：

**1. 慢启动**

- **目的**：在连接刚开始或发现网络拥塞（超时）后，**以一种指数增长的激进方式快速探测网络的可用带宽容量**。
- **机制**：
  - 维护一个**拥塞窗口**，记作 `cwnd`。它代表了在未收到确认的情况下，发送方能发送的最大数据量。`真实发送窗口 = min(拥塞窗口cwnd, 接收方通告窗口awnd)`。
  - 开始时，`cwnd`被设为一个很小的值（如1个MSS）。
  - 每收到一个**ACK**，`cwnd`就**增加1个MSS**。这导致在一个RTT（往返时间）内，`cwnd`会翻倍（指数增长）。
- **结束条件**：
  - **发生超时**：认为网络可能已拥塞，会触发**拥塞避免**。
  - **到达慢启动阈值**：当 `cwnd`增长到一个阈值（`ssthresh`）时，进入**拥塞避免**阶段。
  - **检测到轻微拥塞**：收到3个重复的ACK（表明有包丢失但网络仍在工作），会触发**快速恢复**。

**2. 拥塞避免**

- **目的**：在慢启动探测到大致可用的带宽容量后，**以一种线性增长的保守方式，缓慢逼近网络的最佳容量点**，避免再次引发拥塞。
- **机制**：
  - 当 `cwnd >= ssthresh`时，进入拥塞避免阶段。
  - 每个RTT时间内，`cwnd`只**增加1个MSS**（即每收到一个ACK，`cwnd = cwnd + 1/cwnd`）。这是一种加法增大（AI）。
- **结束条件**：一旦检测到拥塞（超时或3个重复ACK），就退出此阶段。

**3. 快速重传**

- **目的**：一种**丢包检测**的优化机制，不必等待超时计时器到期，从而更快地修复丢包。
- **机制**：
  - 如果发送方连续收到**3个或以上重复的ACK**（意味着接收方一直在索要同一个丢失的包，而中间的包都收到了），它就推断这个数据包很可能丢失了（而不是延迟）。
  - 发送方于是**立即重传**那个被认为丢失的数据包，而不必等待超时。

**4. 快速恢复**

- **目的**：通常与快速重传结合使用。在快速重传之后，**不是像超时那样激进地回到慢启动，而是温和地降低发送速率**，并保持在拥塞避免阶段。
- **机制**（经典TCP Tahoe/Reno算法）：
  - 当收到3个重复ACK，触发快速重传时：
    1. 将 `ssthresh`设置为当前 `cwnd`的一半（乘法减小）。
    2. 然后，**不是将 `cwnd`设为1**，而是将其设为 `ssthresh + 3`（因为3个重复ACK意味着有3个包已离开网络）。
    3. 此后，每收到一个重复ACK，`cwnd`就增加1个MSS（向网络中注入新包）。
    4. 当收到一个确认新数据的ACK时，将 `cwnd`设为 `ssthresh`的值，然后进入**拥塞避免**阶段。

---

### SYN攻击

[4.1 TCP 三次握手与四次挥手面试题 | 小林coding](https://xiaolincoding.com/network/3_tcp/tcp_interview.html#什么是-syn-攻击-如何避免-syn-攻击)

**为什么 TCP 层还需要 MSS ？**

IP不是会分片吗，那还要MSS干什么：

IP分片的代价非常大，完整TCP报文被拆成多个多个 IP 数据包，每个包都要独立封装、发送、路由，增加了处理开销；如果一个分片丢了，那么整个报文都要重传。并且分片可能走不同路径，容易丢/乱序；重组由接收端 IP 层完成，增加 CPU 和内存负担；IP 分片可以被利用进行攻击，比如“分片重组攻击”或“DoS 攻击”

所以TCP的MSS的目的就在于主动避免IP分片：发送方会控制每个 TCP 报文段大小不超过对方的 MSS，从而尽量避免 IP 层发生分片。

---

### accept

`accept`是 TCP 服务器端的一个**核心系统调用**。它的主要职责是**从已完成连接队列中取出一个已经建立好的 TCP 连接**，并为这个新连接创建一个全新的**套接字**，服务器随后通过这个新套接字与客户端进行通信。

`ccept`并不参与 TCP 三次握手的过程，握手是由内核的 TCP/IP 协议栈自动完成的。`accept`只是从队列中取出已经握手成功的连接。

---

## UDP

UDP 是一个简单的、无连接的、不可靠的传输层协议。它只提供最基本的数据传输功能，不保证数据包的顺序、交付或避免重复，因此开销极小、延迟很低，非常适合那些对实时性要求高于可靠性的应用。

第一是**无连接**。通信之前不需要像TCP那样进行三次握手来建立连接，这就意味着它没有建立连接的延迟，可以直接发送数据，效率很高。

第二是**不可靠交付**。它不保证数据包一定能到达对端，如果中间发生丢包、乱序或者错误，UDP自身没有重传机制来纠正，数据包会被直接丢弃。

第三是**无顺序保证**。发送端依次发送的数据包，在接收端可能会以不同的顺序到达，UDP不会对这些数据包进行排序整理。

第四是**面向数据报**。它有消息边界，发送端调用几次`sendto`，接收端就需要调用几次`recvfrom`来接收，不会出现粘包问题。

正因为这些特点，UDP的**头部开销非常小，只有8个字节**，而且它**没有拥塞控制**，会以恒定的速率发送数据。”

“基于这些特性，UDP非常适合那些对**实时性要求高、但可以容忍少量数据丢失**的场景。比如：

- **音视频流媒体和实时通信**，像视频会议、直播、网络电话，偶尔的卡顿或花屏比严重的延迟更容易接受。
- **DNS域名解析**，因为查询请求很小，用UDP一次请求一次响应，效率远高于建立TCP连接。
- **广播和组播**应用，比如DHCP获取IP地址，因为UDP天然支持一对多通信。
- 此外，现在非常流行的**QUIC协议**（HTTP/3的底层基础）也是在UDP之上实现了自己的可靠传输机制，这展现了UDP作为底层构建块的灵活性。”

---

### 怎么用udp实现http

UDP 是不可靠传输的，但基于 UDP 的 **QUIC 协议** 可以实现类似 TCP 的可靠性传输，在http3 就用了 quic 协议。

- 连接迁移：QUIC支持在网络变化时快速迁移连接，例如从WiFi切换到移动数据网络，以保持连接的可靠性。
- 重传机制：QUIC使用重传机制来确保丢失的数据包能够被重新发送，从而提高数据传输的可靠性。
- 前向纠错：QUIC可以使用前向纠错技术，在接收端修复部分丢失的数据，降低重传的需求，提高可靠性和传输效率。
- 拥塞控制：QUIC内置了拥塞控制机制，可以根据网络状况动态调整数据传输速率，以避免网络拥塞和丢包，提高可靠性。

---

## IP

**IP（Internet Protocol，网际协议）** 是 TCP/IP 协议中最重要的协议之一，属于网络层的协议，主要作用是定义数据包的格式、对数据包进行路由和寻址，以便它们可以跨网络传播并到达正确的目的地。

### IP地址

IP 地址（Internet Protocol Address）是 **分配给每台网络设备的唯一标识**，用于在网络中定位和通信。

IP 地址有两种类型：

1. **IPv4**：常见的格式是 `xxx.xxx.xxx.xxx`，每段 0~255，共 32 位，最多约 42 亿个地址，目前已趋近枯竭。
2. **IPv6**：为解决 IPv4 地址不足，IPv6 使用 128 位地址，格式如 `2001:0db8:85a3::8a2e:0370:7334`，地址空间几乎无限。

### IP地址过滤

**IP 地址过滤（IP Address Filtering）** 简单来说就是限制或阻止特定 IP 地址或 IP 地址范围的访问。例如，你有一个图片服务突然被某一个 IP 地址攻击，那我们就可以禁止这个 IP 地址访问图片服务。

IP 地址过滤是一种简单的网络安全措施，实际应用中一般会结合其他网络安全措施，如认证、授权、加密等一起使用。单独使用 IP 地址过滤并不能完全保证网络的安全。

### IP 寻址

**寻址流程简要描述**：

1. **检查是否是同一网段**：
    发送方通过子网掩码判断目标 IP 是否与自己处于同一网段。
   - 如果是：直接通过局域网 ARP 协议获取目标设备的 MAC 地址，完成通信。
   - 如果不是：将数据包交给默认网关（通常是路由器）转发。
2. **路由转发**：
    如果目标 IP 不在本地网段，数据包会被交给路由器，路由器根据自己的**路由表**决定下一跳，将数据一跳一跳转发，直到到达目标网络。
3. **最终送达主机**：
    到达目标网络后，最后一跳路由器使用 ARP 找到目标主机的 MAC 地址，完成局域网传输。

### IPv4 vs IPv6

#### 主要区别

IPv4 和 IPv6 是两种不同版本的 IP 协议，它们的**主要区别**如下：

1. **地址长度不同**
   - IPv4 使用 **32 位地址**，以点分十进制表示，如 `192.168.0.1`，最多约 42 亿个地址；
   - IPv6 使用 **128 位地址**，以冒号分隔的十六进制表示，如 `2001:0db8::1`，地址空间几乎无限。
2. **地址耗尽问题**
   - IPv4 地址数量有限，已接近枯竭；
   - IPv6 地址极其丰富，可满足未来长期需求。
3. **报文结构不同**
   - IPv6 头部更简洁，设计更高效，易于路由器处理；
   - IPv4 报文头较复杂，有更多字段。
4. **NAT（地址转换）**
   - IPv4 中广泛使用 NAT 解决地址不足；
   - IPv6 不需要 NAT，每个设备可拥有唯一公网地址。
5. **安全性和功能扩展**
   - IPv6 内置了 IPsec 加密支持，增强安全性；
   - IPv4 支持较弱，依赖额外配置。
6. **部署现状**
   - IPv4 是当前主流；
   - IPv6 正在逐步推广，特别在移动网络和云计算中增长较快。

#### IPv6主要优势

IPv6 相较于 IPv4，具备以下几个主要优势：

1. **地址空间极大**
    IPv6 使用 128 位地址，支持约 21282^{128}2128 个地址，**几乎可以为地球上每一个设备分配一个全球唯一的公网地址**，彻底解决 IPv4 地址枯竭问题。
2. **不再依赖 NAT**
    由于地址充足，IPv6 允许设备直接通过公网地址通信，**避免了 IPv4 中因 NAT 带来的连接复杂、端到端通信受限等问题**。
3. **更高效的路由与转发**
    IPv6 报文头部设计简洁、固定长度，有利于网络设备更快地解析和转发，提高路由效率。
4. **内置安全机制（IPsec）**
    IPv6 原生支持 IPsec 协议，**提供端到端的数据加密与身份认证**，增强网络通信的安全性。
5. **自动配置支持（无状态地址配置）**
    设备接入网络时可通过 SLAAC 自动生成 IPv6 地址，**无需手动配置或依赖 DHCP**，简化网络管理。
6. **更强的扩展性和服务质量支持**
    IPv6 增强了对多播、QoS（服务质量）、移动性等功能的支持，更适应现代网络应用。

## URL

URL（Uniform Resource Locators），即统一资源定位器。

### 组成结构

```text
协议://主机:端口/路径?查询参数#片段标识
https://www.example.com:443/docs/index.html?lang=en#top
```

它的组成可以解释为：

1. **协议（Scheme）**：`https` 表示使用的是安全的超文本传输协议；
2. **主机（Host）**：`www.example.com` 是服务器的域名，也可以是 IP 地址；
3. **端口（Port）**：`443` 是服务器监听的端口，HTTPS 的默认端口是 443，HTTP 是 80；
4. **路径（Path）**：`/docs/index.html` 指定服务器上资源的具体位置；
5. **查询参数（Query）**：`lang=en` 是向服务器传递的参数，多个参数用 `&` 分隔；
6. **片段标识符（Fragment）**：`#top` 用于标识页面中的某个位置，浏览器使用，不会发送给服务器。

### URI vs URL

- URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。
- URL(Uniform Resource Locator) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。

URI 的作用像身份证号一样，URL 的作用更像家庭住址一样。URL 是一种具体的 URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。

## ARP 协议

ARP（Address Resolution Protocol，地址解析协议）是用于 **将网络层的 IP 地址映射到数据链路层的 MAC 地址** 的协议。它工作在 **数据链路层**，在局域网（LAN）中非常重要。

ARP 工作在 TCP/IP 的**网络接口层**（相当于 OSI 的数据链路层），和 IP 协议配合使用。

ARP 协议的工作原理：

1. **发送 ARP 请求**：
    当主机 A 想与主机 B 通信时，A 已经知道 B 的 **IP 地址**，但不知道 B 的 **MAC 地址**。此时，A 会发送一个 ARP 请求广播到局域网，内容是询问“谁拥有这个 IP 地址？”（目标 IP 地址）。
2. **ARP 响应**：
    网络中的所有设备都会收到这个 ARP 请求，但只有目标 IP 地址匹配的设备（主机 B）会进行响应。主机 B 会通过 ARP 响应消息将自己的 MAC 地址告诉主机 A。
3. **缓存 ARP 响应**：
    主机 A 接收到 ARP 响应后，会将主机 B 的 **IP 地址** 和 **MAC 地址** 记录到 ARP 缓存中。接下来，A 就可以直接通过记录的 MAC 地址与 B 进行通信，而不需要再次发送 ARP 请求。
4. **ARP 请求与响应的格式**：
   - ARP 请求和响应都包含发送方和接收方的 IP 地址、MAC 地址等信息。
   - ARP 请求是一个广播帧，ARP 响应则是单播的。

### ARP 缓存：

ARP 缓存是操作系统用来存储 IP 地址到 MAC 地址映射的表。当需要通信时，系统会查询这个缓存表，避免每次都发送 ARP 请求。如果映射项在缓存中存在且未过期，系统直接使用缓存的 MAC 地址。

### ARP 的常见问题：

- **ARP 欺骗（ARP Spoofing）**：恶意用户可以伪造 ARP 响应，将自己的 MAC 地址关联到一个合法的 IP 地址，从而进行 **中间人攻击** 或数据劫持。ARP 欺骗是局域网中常见的安全隐患之一。

## NAT 协议

[NAT 协议详解（网络层） | JavaGuide](https://javaguide.cn/cs-basics/network/nat.html)

**NAT** 是一种 **网络地址转换** 技术，主要用于 **将私有网络中的 IP 地址转换为公有网络中的 IP 地址**，或者反之。NAT 主要用于解决 **IP 地址不足** 和 **内部网络隐蔽性** 问题，尤其是在 **局域网（LAN）与广域网（WAN）之间**的通信中。

NAT 运行在**网络层附近**，但它不是某个数据包中明确标识的协议，更像是网络设备做的“动作”。

### NAT 的工作原理：

NAT 通常由路由器或防火墙设备实现。当私有网络中的设备需要访问外部网络时，NAT 会根据预设的规则，对请求包的源地址进行转换，将源地址从私有 IP 地址改为公有 IP 地址，并在返回时进行相反的转换。

1. **私有 IP 地址访问外部网络**：
    内部设备通过私有 IP 地址发起网络请求，NAT 设备（如路由器）接收到请求后，将私有 IP 地址转换为公有 IP 地址，然后将数据包转发到目标服务器。
2. **外部响应转换为私有地址**：
    外部服务器响应时，返回的是公有 IP 地址。NAT 设备通过记录的映射关系（通常是通过 **连接跟踪表**），将返回的数据包的目标地址转换为相应的私有 IP 地址，并将数据包发送给内部设备。

### NAT 的类型：

1. **静态 NAT（Static NAT）**：
   - 静态 NAT 将一个私有 IP 地址固定映射到一个公有 IP 地址。每个私有 IP 地址对应一个公有 IP 地址，转换是固定的。
   - 适用于服务器或固定外部访问的设备。
2. **动态 NAT（Dynamic NAT）**：
   - 动态 NAT 在私有 IP 地址和公有 IP 地址池之间建立映射。当内部设备需要外部访问时，NAT 从公有 IP 池中分配一个公有 IP 地址。
   - 不同的内部设备可以共享同一个公有 IP 地址，适合大多数临时访问场景。
3. **端口地址转换（PAT，Port Address Translation）**：
   - PAT 是最常见的 NAT 类型，也叫做 **NAT Overloading**。它允许多个私有 IP 地址共享一个公有 IP 地址。通过不同的端口号来区分不同的连接。
   - 比如，多个内部设备（IP 地址）通过同一个公有 IP 地址访问互联网，每个连接使用不同的端口号进行区分。
   - 这使得在有限的公有 IP 地址池下，可以支持更多的内部设备访问外部网络。

### NAT 的优缺点：

**优点**：

- **节省 IP 地址**：NAT 可以让多个设备共享一个公有 IP 地址，缓解了 IPv4 地址短缺的问题。
- **提高安全性**：通过隐藏内部网络的 IP 地址，外部网络无法直接访问内部设备，提供了一定的安全性。

**缺点**：

- **破坏端到端通信**：NAT 会修改 IP 地址和端口号，可能导致一些基于 IP 地址的协议或服务（如 P2P、IPsec 等）出现问题。
- **复杂性**：NAT 需要维护映射表，可能增加网络配置的复杂性，且可能影响某些实时应用（如 VoIP）。

----

## ARQ 协议

ARQ（Automatic Repeat reQuest，自动重传请求）协议是一种用于确保数据传输可靠性的协议，特别是在存在数据丢失或错误的通信环境中。它的核心功能是通过**自动请求重传丢失或损坏的数据包**，确保数据能够正确、完整地传输。

ARQ 是一种可靠性机制，在传输层（如 TCP）或数据链路层都可能使用。

### ARQ 的基本工作原理：

1. **发送方发送数据**：发送方将数据分段后发送给接收方。
2. **接收方接收并校验数据**：
   - 如果数据包没有错误，接收方发送一个确认消息（ACK）给发送方。
   - 如果数据包有错误或丢失，接收方会请求重传，通常通过发送负确认消息（NACK）或者不发送确认来表示。
3. **重传机制**：如果发送方未收到确认（ACK）或收到负确认（NACK），它会重新发送该数据包，直到数据成功接收并确认。

### ARQ 协议的类型：

1. **停等 ARQ（Stop-and-Wait ARQ）**：每次只发送一个数据包，必须等待接收方确认后才能发送下一个数据包。简单但效率较低，特别是在高延迟网络中。
2. **连续 ARQ（Continuous ARQ）**：发送方可以连续发送多个数据包，接收方确认每个数据包，丢失或错误的数据包才会被重传。常见的实现包括：
   - **Go-Back-N ARQ**：接收方在丢失数据包时会丢弃所有后续数据包，需要重新发送丢失的那个包及其后的所有包。
   - **Selective Repeat ARQ**：只重传丢失或错误的包，其他未丢失的包不受影响。

## 网络攻击

### DDOS攻击

**分布式拒绝服务**。指的是处于不同位置的多个攻击者同时向一个或数个目标发动攻击，是一种分布的、协同的大规模攻击方式。单一的 DoS 攻击一般是采用一对一方式的，它利用网络协议和操作系统的一些缺陷，采用**欺骗和伪装**的策略来进行网络攻击，使网站服务器充斥大量要求回复的信息，消耗网络带宽或系统资源，导致网络或系统不胜负荷以至于瘫痪而停止提供正常的网络服务。

---

#### 分类

常见的DDoS攻击包括以下几类：

- **网络层攻击**：比较典型的攻击类型是UDP反射攻击，例如：NTP Flood攻击，这类攻击主要利用大流量拥塞被攻击者的网络带宽，导致被攻击者的业务无法正常响应客户访问。
- **传输层攻击**：比较典型的攻击类型包括SYN Flood攻击、连接数攻击等，这类攻击通过占用服务器的连接池资源从而达到拒绝服务的目的。
- **会话层攻击**：比较典型的攻击类型是SSL连接攻击，这类攻击占用服务器的SSL会话资源从而达到拒绝服务的目的。
- **应用层攻击**：比较典型的攻击类型包括DNS flood攻击、HTTP flood攻击、游戏假人攻击等，这类攻击占用服务器的应用处理资源极大的消耗服务器处理性能从而达到拒绝服务的目的。

---

#### **如何应对**

应对 DDoS 攻击的关键在于**提前防御 + 实时检测 + 快速缓解**。针对不同类型的 DDoS 攻击，应采取分层防护策略，主要包括以下几方面：

**第一，接入层限流与黑名单过滤。**
 在接入层（如负载均衡器、防火墙或 CDN）对异常流量进行限速、封禁可疑 IP、过滤无效请求。例如 SYN Flood 攻击，可以通过启用 SYN Cookie 或连接速率限制来抵御。

**第二，使用抗 DDoS 服务。**
 对于大规模攻击，企业自身往往难以承受，可以接入第三方抗 DDoS 平台或云厂商的清洗服务，如阿里云、腾讯云、Cloudflare、Akamai 等。这些平台具备大规模流量牵引和清洗能力。

**第三，加固服务器和网络架构。**
 包括部署负载均衡、使用多节点或多机房架构，将服务部署在弹性集群上，提高系统的抗压能力。此外，还可以通过分布式部署和故障转移机制提升可用性。

**第四，监控与自动化响应机制。**
 建立实时流量监控系统，一旦发现突发异常流量或资源占用激增，自动触发报警和限流、切换策略等应急措施，减少人工介入时间。

**最后，提前准备应急预案。**
 针对不同攻击类型制定完整的应急方案，包括隔离攻击源、快速更换 IP、通知上游运营商等操作，确保在攻击发生时可以快速反应。

---

### CSRF攻击

CSRF（跨站请求伪造）是一种攻击手段，攻击者通过诱导用户执行恶意操作，从而获取用户数据或执行恶意代码。CSRF攻击通常通过伪造一个合法的HTTP请求来实现，这个请求看起来是合法的，但实际上是为了执行一个攻击者控制的操作。

#### 解决方法

1. 验证用户会话：在服务器端对用户会话进行验证，确保请求的会话标识符与当前会话标识符匹配。这样可以防止攻击者伪造会话标识符。
2. 使用双重验证：除了会话验证，还可以使用其他验证方式，例如验证码、签名验证等。这些验证方式可以增加攻击的难度。
3. 防止跨站请求：通过设置CSP（内容安全策略）来防止跨站请求，限制网页中可执行的脚本源，减少攻击者诱导用户执行恶意操作的可能性。
4. 避免使用自动提交表单：禁用默认的自动提交功能，要求用户在提交表单前确认操作，防止攻击者诱导用户在未经授权的情况下提交表单。
5. 强制Referer头部：在服务器端检查请求的Referer头部，确保请求来自可信来源。

---

### XSS攻击

XSS是跨站脚本攻击，攻击者通过在Web页面中插入恶意脚本代码，然后诱使用户访问该页面，从而使得恶意脚本在用户浏览器中执行，从而盗取用户信息、会话信息等敏感数据，甚至控制用户账户。

#### 分类

XSS 攻击可以分为 3 类：存储型（持久型）、反射型（非持久型）、DOM 型。

- [存储型 XSS (opens new window)](https://developer.mozilla.org/zh-CN/docs/Glossary/Cross-site_scripting#存储型_xss)：注入型脚本永久存储在目标服务器上。当浏览器请求数据时，脚本从服务器上传回并执行。
- [反射型 XSS (opens new window)](https://developer.mozilla.org/zh-CN/docs/Glossary/Cross-site_scripting#反射型_xss)：当用户点击一个恶意链接，或者提交一个表单，或者进入一个恶意网站时，注入脚本进入被攻击者的网站。Web 服务器将注入脚本，比如一个错误信息，搜索结果等 返回到用户的浏览器上。由于浏览器认为这个响应来自"可信任"的服务器，所以会执行这段脚本。
- [基于 DOM 的 XSS (opens new window)](https://developer.mozilla.org/zh-CN/docs/Glossary/Cross-site_scripting#基于_dom_的_xss)：通过修改原始的客户端代码，受害者浏览器的 DOM 环境改变，导致有效载荷的执行。也就是说，页面本身并没有变化，但由于 DOM 环境被恶意修改，有客户端代码被包含进了页面，并且意外执行。

---

#### 预防方法

- 输入验证：对所有用户输入的数据进行有效性检验，过滤或转义特殊字符。例如，禁止用户输入HTML标签和JavaScript代码。
- 输出编码：在网页输出用户输入内容时，使用合适的编码方式，如HTML转义、URL编码等，防止恶意脚本注入。
- Content Security Policy（CSP）：通过设置CSP策略，限制网页中可执行的脚本源，有效防范XSS攻击。
- 使用HttpOnly标记：在设置Cookie时，设置HttpOnly属性，使得Cookie无法被JavaScript代码读取，减少受到XSS攻击的可能。

---

### SYN Flood

**SYN Flood 是一种典型的 DDoS 攻击方式，属于资源消耗型攻击，针对的是 TCP 协议的三次握手机制。**

在正常的 TCP 建立连接过程中，客户端先发送 SYN 报文，服务器收到后会分配资源并返回 SYN-ACK，最后客户端再发送 ACK，连接才正式建立。

但在 SYN Flood 攻击中，**攻击者大量伪造源 IP 的 SYN 请求**发送给服务器，而不完成最后一步的 ACK。这样，服务器会不断地为这些“半开连接”分配资源并等待客户端确认，直到连接超时。

当攻击量足够大时，服务器的连接队列（如 TCP 半连接队列）会被填满，导致无法处理新的正常连接，从而使服务陷入瘫痪。

------

#### 常见防御手段包括：

1. **启用 SYN Cookie**，不立刻为半连接分配资源，而是将状态信息编码进初始序列号中，等客户端确认后再真正建立连接。
2. **缩小半连接队列超时时间**，减少资源被占时间。
3. **限制单个 IP 的连接频率**，防止恶意刷请求。
4. **接入高防或抗 DDoS 服务**，将攻击流量在源头进行清洗。

### UDP Flood

**UDP Flood 是一种基于 UDP 协议的 DDoS 攻击方式，攻击目标是消耗服务器的处理能力和带宽资源。**

由于 UDP 是无连接的协议，发送方无需建立连接即可不断发送数据包。因此在 UDP Flood 攻击中，**攻击者持续向目标服务器发送大量伪造的 UDP 数据报**，通常是随机端口号的数据。服务器在接收到这些请求后，会尝试：

- 检查目标端口是否开放；
- 如果端口未开放，还可能返回一个 ICMP 不可达响应。

这种处理会占用大量计算资源和网络带宽，尤其在大流量攻击下，可能导致服务器资源耗尽或网络拥塞，最终导致服务不可用。

------

#### 防御措施包括：

1. **限制 UDP 速率**：在防火墙、网关或操作系统层面配置对 UDP 流量的速率限制。
2. **启用智能流量过滤**：通过 DPI（深度包检测）识别异常 UDP 流量特征。
3. **接入抗 DDoS 清洗服务**：在云平台或运营商层引流清洗大规模 UDP 攻击流量。
4. **禁用不必要的 UDP 服务**：减少攻击面，例如关闭不使用的 TFTP、DNS、NTP 等端口。

### HTTP Flood 

**HTTP Flood 是一种应用层 DDoS 攻击，攻击者伪装成正常用户，通过持续发送大量合法的 HTTP 请求来压垮服务器。**

与传统的网络层攻击（如 SYN Flood、UDP Flood）不同，HTTP Flood 不依赖伪造 IP，也不直接消耗带宽，而是通过不断请求目标网站的页面、接口或资源来**消耗服务器的 CPU、内存和数据库连接等后端资源**。

攻击请求往往看起来非常“正常”，可能是访问首页、搜索接口、提交表单等，这使得它很难通过传统的黑名单或协议规则过滤。

------

#### 特点：

- **高隐蔽性**：请求形式和路径与正常用户几乎一致；
- **低带宽高消耗**：相比网络层攻击，占带宽不大，但对服务器压力极高；
- **攻击效果强**：容易绕过基础防火墙，直接影响 Web 服务稳定性。

------

#### 防御手段包括：

1. **行为分析与挑战机制**：引入验证码（如图形验证码、JS 校验）过滤掉非人类请求；
2. **接入 Web 应用防火墙（WAF）**：识别异常访问行为和请求模式；
3. **使用 CDN 进行缓存和流量缓冲**：减少源站压力；
4. **IP/UA 访问频率限制**：针对高频访问 IP 做封禁或限流。

### DNS Flood

**DNS Flood 是一种针对域名系统（DNS）的 DDoS 攻击，攻击者向目标 DNS 服务器发送大量请求，试图耗尽其处理能力，从而导致域名解析服务不可用。**

DNS 是互联网的“电话簿”，将域名解析为 IP 地址，一旦 DNS 服务瘫痪，用户将无法访问网站或服务。攻击者通过构造大量合法或伪造的 DNS 查询请求，向目标服务器持续施压，导致其资源耗尽，无法响应正常用户请求。

------

#### 特点：

- **目标明确**：攻击 DNS 服务，使整个网站不可访问；
- **速率高、请求轻**：单个 DNS 查询数据量小，但高频请求会迅速压垮服务器；
- **可结合反射放大攻击**：攻击者伪造源 IP，将请求发送给开放的 DNS 服务器，使其响应放大后打向目标，效果更猛烈。

------

#### 防御手段包括：

1. **启用速率限制**：限制单位时间内每个 IP 的 DNS 查询次数；
2. **使用 Anycast 和多节点分发**：将请求分流到多个地理节点，缓解单点压力；
3. **接入云 DNS 或防护平台**：借助第三方清洗服务抵御大规模攻击；
4. **关闭递归查询（对外）**：避免 DNS 被用于反射攻击；
5. **缓存优化**：提高命中率，减少服务器重复计算压力。

### 中间人攻击

**中间人攻击是一种典型的被动+主动结合的网络攻击方式，攻击者通过“夹在通信双方中间”，拦截、篡改或伪造双方传输的数据，从而窃取敏感信息或干扰通信内容。**

这种攻击一般发生在客户端与服务器之间的数据传输过程中，尤其在通信未加密或加密方式被破坏的情况下更容易发生。

------

#### 常见的中间人攻击方式包括：

1. **Wi-Fi 嗅探**：攻击者搭建恶意热点或监听公共 Wi-Fi，截获明文传输数据；
2. **ARP 欺骗**：在局域网中伪装为网关，拦截局域网中的通信；
3. **DNS 劫持**：篡改域名解析结果，将用户引导到钓鱼网站；
4. **SSL 劫持**：伪造证书，欺骗客户端建立“假”的 HTTPS 连接，窃取加密数据。

------

#### 危害包括：

- 用户账户、密码、身份证号等敏感信息泄露；
- 会话被劫持，用户以为自己在和目标网站通信，实际是与攻击者通信；
- 通信数据被修改，例如转账金额、验证码等。

------

#### 防御措施：

1. **使用 HTTPS + 合法数字证书**，保障通信加密和身份认证；
2. **启用 HSTS（HTTP Strict Transport Security）**，强制浏览器使用 HTTPS；
3. **验证证书合法性**，防止伪造证书欺骗；
4. **避免连接不可信 Wi-Fi 网络**，提升客户端安全意识；
5. **在局域网中启用 ARP 防护和网络隔离**，防止本地伪造。

### TCP重置攻击

**TCP 重置攻击是一种利用 TCP 协议特性，强行中断正常连接的攻击方式。**

在 TCP 协议中，通信双方通过三次握手建立连接后，通过发送数据并维持状态。而如果任意一方收到一个带有 **RST（Reset）标志位**的 TCP 报文，就会立即断开连接，释放资源，不再继续通信。

攻击者正是利用这一点，通过伪造源地址、端口以及 TCP 序列号，向通信的一方伪造发送一个 **带 RST 标志的 TCP 报文**，使其误以为对方要求断开连接，从而造成通信中断。

------

#### 攻击特点：

- **无需占用目标资源**，只需构造一个伪造的 RST 包即可终止连接；
- **攻击隐蔽**，数据包看起来像合法中断；
- **高危场景**：VPN、数据库、SSH 远程连接、长时间保持的 TCP 连接。

------

#### 发起条件：

攻击者必须**监听到或预测到 TCP 序列号和连接信息（源/目标 IP + 端口号）**，因此通常在以下场景下可能发生：

- 攻击者处于局域网或中间网络路径中；
- 应用未启用加密保护（如 TLS）；
- 存在信息泄露或默认端口可预测。

------

#### 防御方式：

1. **使用 TLS/SSL 加密通信**，即使连接被断，也能校验完整性，避免数据丢失或伪造；
2. **使用更随机的初始序列号**，增加攻击者预测难度；
3. **在防火墙或网关中启用 TCP 检查机制**，过滤伪造 RST 包；
4. **启用 IDS/IPS 检测**，发现频繁或异常的 RST 包行为。

### IP欺骗

**IP 欺骗是一种伪造 IP 数据包源地址的攻击技术，攻击者通过将自己的 IP 地址伪装成另一个主机的地址，使得目标主机认为数据包是从一个信任的来源发送的。**

在正常的数据传输过程中，数据包的源 IP 地址会告诉接收方是谁发送了数据。而在 IP 欺骗中，攻击者**伪造源 IP 地址**，使得接收方误以为数据来自可信的源。

------

#### IP 欺骗的目的：

1. **隐藏真实身份**：攻击者可以隐藏自己的身份，避免被追踪。
2. **绕过身份验证**：通过伪装成受信任的主机，突破防火墙或安全认证机制。
3. **发起其他攻击**：例如 SYN Flood、DDoS 攻击等，攻击者利用伪造的 IP 地址将流量分散或隐藏攻击来源。

------

#### 攻击特点：

- **伪造简单**：攻击者只需要修改数据包的源 IP 地址即可。
- **难以追踪**：由于源 IP 地址被伪造，目标无法直接追踪到攻击者的真实位置。
- **依赖于网络拓扑和协议漏洞**：攻击通常针对特定协议（如 TCP）和网络环境进行，利用目标网络的特定漏洞。

------

#### 防御手段：

1. **启用入站和出站过滤（ACL）**：通过访问控制列表（ACL）限制不可信的 IP 地址。
2. **启用 IPsec**：通过加密和认证机制确保通信的真实性和完整性。
3. **使用反向路径过滤（RPF）**：检查返回路径，确保接收到的 IP 地址是合法的。
4. **数据包完整性验证**：使用防火墙和 IDS/IPS 系统，检测和阻止伪造的数据包。

## 其他

### MAC 地址

MAC 地址的全称是 **媒体访问控制地址（Media Access Control Address）**。如果说，互联网中每一个资源都由 IP 地址唯一标识（IP 协议内容），那么一切网络设备都由 MAC 地址唯一标识。

MAC 地址的长度为 6 字节（48 比特），地址空间大小有 280 万亿之多（$2^{48}$），MAC 地址由 IEEE 统一管理与分配，理论上，一个网络设备中的网卡上的 MAC 地址是永久的。不同的网卡生产商从 IEEE 那里购买自己的 MAC 地址空间（MAC 的前 24 比特），也就是前 24 比特由 IEEE 统一管理，保证不会重复。而后 24 比特，由各家生产商自己管理，同样保证生产的两块网卡的 MAC 地址不会重复。

MAC 地址具有可携带性、永久性，身份证号永久地标识一个人的身份，不论他到哪里都不会改变。而 IP 地址不具有这些性质，当一台设备更换了网络，它的 IP 地址也就可能发生改变，也就是它在互联网中的定位发生了变化。

最后，记住，MAC 地址有一个特殊地址：FF-FF-FF-FF-FF-FF（全 1 地址），该地址表示广播地址。

### PING

PING 命令是一种常用的网络诊断工具，经常用来测试网络中主机之间的连通性和网络延迟。

PING 命令的输出结果通常包括以下几部分信息：

1. **ICMP Echo Request（请求报文）信息**：序列号、TTL（Time to Live）值。
2. **目标主机的域名或 IP 地址**：输出结果的第一行。
3. **往返时间（RTT，Round-Trip Time）**：从发送 ICMP Echo Request（请求报文）到接收到 ICMP Echo Reply（响应报文）的总时间，用来衡量网络连接的延迟。
4. **统计结果（Statistics）**：包括发送的 ICMP 请求数据包数量、接收到的 ICMP 响应数据包数量、丢包率、往返时间（RTT）的最小、平均、最大和标准偏差值。

如果 PING 对应的目标主机无法得到正确的响应，则表明这两个主机之间的连通性存在问题（有些主机或网络管理员可能禁用了对 ICMP 请求的回复，这样也会导致无法得到正确的响应）。如果往返时间（RTT）过高，则表明网络延迟过高。

**工作原理**

PING 基于网络层的 **ICMP（Internet Control Message Protocol，互联网控制报文协议）**，其主要原理就是通过在网络上发送和接收 ICMP 报文实现的。

ICMP 报文中包含了类型字段，用于标识 ICMP 报文类型。ICMP 报文的类型有很多种，但大致可以分为两类：

- **查询报文类型**：向目标主机发送请求并期望得到响应。
- **差错报文类型**：向源主机发送错误信息，用于报告网络中的错误情况。

PING 用到的 ICMP Echo Request（类型为 8 ） 和 ICMP Echo Reply（类型为 0） 属于查询报文类型 。

- PING 命令会向目标主机发送 ICMP Echo Request。
- 如果两个主机的连通性正常，目标主机会返回一个对应的 ICMP Echo Reply。

### 对称加密 vs 非对称加密

对称加密和非对称加密是现代密码学的两种核心机制，它们的根本区别在于加密和解密所使用的密钥是否相同。

对称加密采用单密钥体系，加密和解密使用同一把密钥。它的优点是计算速度非常快，效率很高，非常适合用来加密大量的实际业务数据。但它的核心缺陷在于密钥分发非常困难，通信双方必须通过一个安全的渠道预先共享同一把密钥，这在实际的互联网环境中很难保证安全，而且密钥的管理和维护成本也会随着通信方数量的增加而急剧上升。

非对称加密则采用了公钥和私钥配对的双密钥体系。用公钥加密的数据只能用对应的私钥解密，反之亦然。它的最大优势是完美解决了密钥分发难题，公钥可以完全公开，发送方直接用接收方的公钥加密数据即可，私钥则始终由接收方自己安全保管。但它的缺点是计算速度非常慢，比对称加密慢好几个数量级，不适合直接加密大量数据。

正因为它们各有优劣，在实际应用中总是相辅相成的。最典型的例子就是HTSSL/TLS协议。在连接建立阶段，使用非对称加密（如RSA）来安全地交换一个临时的会话密钥，这个过程解决了最关键的密钥分发问题。随后，双方就使用这个会话密钥切换到对称加密算法（如AES）来加密所有后续的通信数据，从而兼顾了安全性和高性能。

总结来说，对称加密是干活的主力，但需要解决如何安全传递工具的问题；非对称加密则是传递工具的安全信使，两者结合共同构建了安全的网络通信基础。

### 从输入`URL`到界面展示

1，对URL解析，确定服务器主机名和请求路径，生成完整HTTP请求

2，查询服务器域名对应的IP地址：也就是域名解析，浏览器检查本地，操作系统检查其DNS缓存，查询本地DNS服务器，递归查询（根DNS服务器-顶级域DNS服务器-权威DNS服务器），返回该域名对应的IP地址给浏览器

3.建立TCP连接：浏览器通过操作系统的Socket接口向目标IP发起TCP连接，TCP三次握手

4.生成HTTP请求报文：请求行、请求头、请求体

5.发送HTTP请求：浏览器通过建立的TCP连接将HTTP请求报文发送到目标服务器。在此过程中，HTTP请求报文会被传递给下层的TCP协议，TCP会将其封装成数据包，并通过**IP层**为其加上IP头。

6.网络层(IP层)的封装与转发：在此过程中，HTTP请求报文会被传递给下层的TCP协议，TCP会将其封装成数据包，并通过**IP层**为其加上IP头。

7.数据链路层(MAC层)：数据包到达目标设备所在网络时，**路由器**通过ARP（地址解析协议）查找目标设备的MAC地址。如果目标设备在同一局域网内，路由器通过ARP获取目标设备的MAC地址，并将IP包封装成以太网帧。交换机使用MAC地址表，将帧转发到正确的端口，确保数据到达目标主机。

8.目标服务器接收数据：目标服务器解析**MAC头**、**IP头**，然后解析**TCP头**来获取数据，服务器检查TCP段的目的端口号，以确定该数据是由哪一个应用程序处理。服务器应用程序（如Web服务器）提取出HTTP请求报文，开始处理请求。

9.服务器生成HTTP响应报文：响应行、响应头、响应体

10.服务器通过TCP连接返回HTTP响应

11.浏览器接收响应，解析HTTP头部，处理响应体内容，渲染网页

12.关闭连接（HTTP/1.1，连接可能保持一段时间但最终会被关闭），TCP四次挥手关闭连接

---

### 应用场景

{% folding child:codeblock open:true color:yellow 网页非常慢转圈圈的时候，要定位问题需要从哪些角度 %}

最直接的办法就是抓包，排查的思路大概有：

1，先确定是服务端的问题，还是客户端的问题。先确认浏览器是否可以访问其他网站，如果不可以，说明客户端网络自身的问题，然后检查客户端网络配置（连接wifi正不正常，有没有插网线）；如果可以正常其他网页，说明客户端网络是可以正常上网的。

2，如果客户端网络没问题，就抓包确认 DNS 是否解析出了 IP 地址，如果没有解析出来，说明域名写错了，如果解析出了 IP 地址，抓包确认有没有和服务端建立三次握手，如果能成功建立三次握手，并且发出了 HTTP 请求，但是就是没有显示页面，可以查看服务端返回的响应码：

- 如果是404错误码，检查输入的url是否正确；
- 如果是500，说明服务器此时有问题；
- 如果是200，F12看看前端代码有问题导致浏览器没有渲染出页面。

3，如果客户端网络是正常的，但是访问速度很慢，导致很久才显示出来。这时候要看客户端的网口流量是否太大的了，导致tcp发生丢包之类的问题。

总之就是一层一层有没有插网线，网络配置是否正确、DNS有没有解析出 IP地址、TCP有没有三次握手、HTTP返回的响应码是什么。

{% endfolding %}



{% folding child:codeblock open:true color:yellow 服务端正常启动了，但是客户端请求不到有哪些原因?如何排查? %}如果客户端请求的接口没有响应，排查的方式：

- 检查接口IP地址是否正确，ping一下接口地址。
- 检查被测接口端口号是否正确，可以在本机Telnet接口的IP和端口号，检查端口号能否连通
- 检查服务器的防火墙是否关闭，如果是以为安全或者权限问题不能关闭，需要找运维进行策略配置，开放对应的IP和端口。
- 检查你的客户端（浏览器、[测试工具 (opens new window)](https://so.csdn.net/so/search?q=测试工具&spm=1001.2101.3001.7020)），是否设置了网络代理，网络代理可以造成请求失败。

如果客户端的请求有响应，但是返回了错误状态码，那么根据错误码做对应的排查：

- 400：客户端请求错误，比如请求参数格式错误
- 401：未授权，比如请求header里，缺乏必要的信息头。（token，auth等）
- 403：禁止，常见原因是因为用户的账号没有对应的URL权限，还有就是项目中所用的中间件，不允许远程连接（Tomcat）
- 404：资源未找到，导致这种情况的原因很多，比如URL地址不正确
- 500：服务器内部错误，出现这种情况，说明服务器内部报错了 ，需要登录服务器，检查错误日志，根具体的提示信息在进行排查
- 502/503/504（错误的网关、服务器无法获得、网关超时）：如果单次调用接口就报该错误，说明后端服务器配置有问题或者服务不可用，挂掉了；如果是并发压测时出现的，说明后端压力太大，出现异常，此问题一般是后端出现了响应时间过长或者是无响应造成的

{% endfolding %}



{% folding child:codeblock open:true color:yellow server a和server b，如何判断两个服务器正常连接？出错怎么办 %}
代码块
{% endfolding %}



{% folding child:codeblock open:true color:yellow 服务器ping不通但是http能请求成功，会出现这种情况吗?什么原因造成的? %}

ping 走的是 icmp 协议，http 走的是 tcp 协议。

有可能服务器的防火墙禁止 icmp 协议，但是 tcp 协议没有禁止，就会出现服务器 ping 不通，但是 http 能请求成果。

{% endfolding %}



{% folding child:codeblock open:true color:yellow 服务端出现大量的timewait有哪些原因 %}

首先需要明确，**`TIME_WAIT`是TCP四次挥手后由主动关闭连接的一方进入的正常状态**，其持续时间通常是 2MSL（报文最大生存时间，在Linux中一般为60秒）。

因此，当服务端出现大量 `TIME_WAIT`，根本原因是**服务端主动断开了大量的TCP连接**。这通常是由以下几个场景触发的：

**1. 服务端主动发起关闭（最常见原因）**

- **HTTP服务场景**：在HTTP 1.1协议中，虽然默认是长连接，但许多服务器框架或配置会在一段时间空闲后、处理完特定数量的请求后、或者在发送错误响应（如4xx, 5xx）后，由**服务端主动发起关闭**。每次这样的关闭都会在服务端产生一个 `TIME_WAIT`。
- **短连接服务**：如果服务是基于短连接的（如某些RPC服务或未启用Keep-Alive的HTTP/1.0），每次请求-响应循环都会经历一次完整的TCP连接建立和断开。服务端处理完请求后主动关闭，就会产生 `TIME_WAIT`。

**2. 负载均衡器或代理后的服务端**

- 在Nginx等反向代理架构中，Nginx作为客户端与后端应用服务（如Tomcat）建立连接。当请求处理完毕，如果由后端的Tomcat主动关闭连接，那么 `TIME_WAIT`就会堆积在Tomcat服务器上，而不是Nginx上。

**3. 连接池配置不当**

- 如果服务端使用连接池访问下游服务（如数据库、Redis），当连接池中的连接因为空闲或异常被主动销毁时，也会在本地产生 `TIME_WAIT`。

**大量 `TIME_WAIT`的潜在影响：**

虽然每个 `TIME_WAIT`状态连接占用的资源很少，但当数量极其巨大时（如数万以上），可能会：

- **耗尽可用端口**：新连接需要源端口，而大量 `TIME_WAIT`连接占用了“源IP+源端口”的资源，可能导致无法创建新连接。
- **占用内核内存**：每个 socket 结构都会占用一定的内存。

------

**常见的解决方案思路：**

1. **调整内核参数（治标）**：
   - **开启端口复用**：`net.ipv4.tcp_tw_reuse`（允许将`TIME_WAIT`连接套接字用于新的出站连接，前提是时间戳开启）。
   - **快速回收**：谨慎调整 `net.ipv4.tcp_tw_recycle`（该选项在现代网络中已不推荐，且在新版内核中已移除）。
   - **增大可用端口范围**：调整 `net.ipv4.ip_local_port_range`。
2. **优化应用设计（治本）**：
   - **使用长连接**：确保HTTP服务正确配置并启用Keep-Alive，避免频繁的连接建立与断开。
   - **变更关闭策略**：在架构设计上，如果可能，**让客户端主动关闭连接**，将 `TIME_WAIT`状态分散到大量的客户端，从而避免在服务端堆积。这是最根本的解决方案。

{% endfolding %}








