---
title : 集合
order: 2
---

## List

#### `ArrayList`

`ArrayList`是基于数组实现的可变长度的列表。它实现了`RadomAccess`标记接口，支持索引快速访问。底层就是一个Object[]数组。线程不安全。

如果你没有指定大小那它是懒加载的，创建了一个`ArrayList`的时候并不会立刻分配数组，而是把内部的`elementDate`指向一个共享的空数组`EMPTY_ELEMENTDATA`，真正的初始化发生在第一次添加元素的时候，这样的目的是为了节省内存。

当你调用add()会进行元素个数和容量的判断，判断是否达到数组容量，如果是就会触发扩容，这里的扩容是一个懒触发的过程，只有在满了的那一刻才扩容，会触发grow()方法，把原数组的容量扩大到1.5倍，实际上是创建了一个新数组然后`Arrays.copyOf()`方法复制的。

至于为什么是1.5倍，从源码上看是因为`oldCapacity + (oldCapacity >> 1);`，>>右移一位就是除以2^1，从设计上讲，这是一个性能和空间的权衡折中方案，因为频繁的扩容会导致性能浪费，而扩太多就会导致空间浪费。

至于插入删除的复杂度，如果是头部和指定位置都是O(n)，如果是尾部插入，又分为需要扩容和不需要扩容的情况，如果需要扩容，那么会新建数组然后`copyOf()`，复杂度是O(n)，如果不需要扩容就是O(1)。

#### `LinkedList`

`LinkedList`是基于双向链表实现的，底层是由多个节点（`Node`）连接成的链表。包括前后节点的引用和存储的元素。线程不安全。

元素的添加都是O(1)，删除头尾也都是O(1)，但是删除中间节点的时候需要定位到节点，所以是O(n)。

并且它实现了 `List`、`Deque` 和 `Queue` 接口，所以既能当列表用，也能当队列、双端队列、栈来用，功能还是很丰富的。

#### `CopyOnWriteArrayList`

`CopyOnWriteArrayList`是Java的一个List的线程安全实现，专门为解决多线程并发场景中的数据一致性的高效读取提供解决方案。

底层是一个`Obejec[]`数组。

核心思想是每次写操作都创建底层数组的一个新副本，修改副本会再将副本替换原数组，复杂度为O(n)。读操作直接访问底层数组，复杂度是O(1)。

优点是写操作创建新数组并替换，不需要额外的同步操作，确保了线程安全。并且读操作非常高效。

缺点是每次写操作都要复制整个数组，频繁写入的情况下不仅写操作的时间复杂度为O(n)性能低，并且内存消耗大。

另外它在某些情况下会导致读出来的数据不是最新的，尤其是在写操作发生时。解决方法就是使用锁机制(如，`synchronized`)或者其他线程安全的集合(如`ConcurrentLinkedQueue`)；

所以它适用于读多写少的场景，以及作为事件监听器。

#### `Collections.synchronizedList()`

`Collections.synchronizedList()` 是 Java 中的一个工具方法，用于将一个普通的 `List` 包装成一个线程安全的 `List`。它是通过对所有的 `List` 操作加锁来保证线程安全的。

## Set

HashSet，LinedHashSet，TreeSet，都是Set接口的实现类，都能保证元素唯一，并且都不是线程安全的。

HashSet实际上就是半个HashMap，它是基于HasmMap的Key唯一的特性实现的，用来实现HashSet的HasmMap的value是一个固定常量，一般是Boolean.TRUE。

LinkedHashSet就是在HashSet的基础上维护了一个双向链表表示数据的插入顺序。

TreeSet是基于TreeMap实现的，底层是红黑树，它对数据满足自然排序或者你提供的Comparator排序。

## Queue

Queue队列

Duque双端队列

## Map

#### HashMap

TreeMap和HashMap都继承自AbstractMap

HashMap的底层是数组+链表/红黑树，核心原理是**通过 key 的 hash 值快速定位元素在数组中的位置**，然后通过 `equals()` 判断是否为同一个 key，实现快速查找和插入；当put插入数据的时候，首先调用hashcode方法计算哈希值，然后hash方法又会对这个哈希值进行扰动处理（本质是添加高位信息的计算，降低低位重复的风险），目的是为了让哈希值更均匀，减少碰撞。然后回对数组长度取模得到数组下标，也就是桶的索引，如果这个位置是空的，就直接在这个位置放一个Node节点（键值对的封装对象），如果已经有数据了，说明发生了哈希冲突，这个时候会遍历桶的链表/红黑树，先用hash筛选可能的key，然后使用equals方法判断是不是同一个key，如果key已经存在，就更新value，反之，就添加这个节点，走链表（8之前头插，之后尾插，性能稍慢但是并发扩容不会造成循环链表）/红黑树的添加逻辑。如果某个桶的链表长度超过8并且总容量大于64，就会转化成红黑树，效率是O（logn）；当查找元素的时候，首先计算key的哈希值，再去桶遍历链表/红黑树，找hash值相等的节点，然后用equals判断是不是目标key，如果找到了就返回value；扩容：HashMap的默认初始容量是16，记载因子是0.75，当元素数量超过16*0.75=12的时候，发生扩容，扩容会把原数组容量扩大为原来的2倍，然后把原来的元素再次hash放到新位置，这一步的开销较大，因为涉及所有元素的重新计算和移动。

优先扩容而不是优先转化为红黑树，因为数组扩容能减少哈希冲突的概率，并且红黑树需要维持自平衡，维护成本较高，并且红黑树会增加复杂度（节点占用内存更大）。

为什么扩容是8和64：因为**泊松分布**表明，链表长度到达8的情况非常非常少。设为8可以保证性能和空间的平衡。64也是实践验证的经验值，优先扩容而不是优先转化红黑树，当数组大小达到64的时候，冲突概率较高，转化成红黑树的性能优势就明显了。

为什么扩容*2：目的是为了让hash计算更加高效；首先，HashMap的容量是2的幂，hash取模可以使用位运算代替，（n-1）& hash；扩容后的再次hash更容易判断元素是否发生位置移动，重新hash后我们只需要看新增的哪一位是0还是1就能判断出来是否需要后移oldCapacity（是1就移）；

为什么HashMap不安全？1.7之前头插法导致在多线程环境下扩容操作可能存在死循环问题；另外无论是7还是8都存在一个数据丢失的问题，实际上是数据覆盖，一种情况是线程1发现hash冲突并且equals满足，即将覆盖数据的时候，CPU执行权被线程2抢走，数据2覆盖了数据，等到线程1再次操作的时候就会把线程2的数据覆盖。还有一种情况是两个线程同时 `put` 操作导致 `size` 的值不正确，进而导致数据覆盖的问题。

#### ConcurreHashMap

ConcurreHashMap是线程安全版的hashmap。

1.8之前，底层采用分段数组+链表，对整合数组进行了分割分段（分段锁），多线程访问不同数据段的数据的时候就不会发生锁竞争。

1.8之后，底层和HashMap一样，都是数组+链表/红黑树，摒弃了分段锁的策略，而是采用了synchronized锁和CAS来操作。锁的颗粒度更细，只锁定当前链表/红黑树的首节点，只要hash不冲突，就不会产生并发，效率大幅提升，并发力度更大。

`HashMap` 的 key 和 value 都能为 null，但 key 最多只能有一个为 null；而 `ConcurrentHashMap` 两者都不允许为 null，因为二义性，你没办法是不存在才返回null还是因为值本身就是null。多线程下无法正确判定键值对是否存在（存在其他线程修改的情况），单线程是可以的（不存在其他线程修改的情况）。



















